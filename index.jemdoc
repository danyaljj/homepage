# jemdoc: menu{MENU}{acc.html}, title{Daniel Khashabi} nofooter

~~~
{}{img_left}{profile11.png}{alt text}{170}{}{}
{{<span style="font-weight: bold; font-size: 125%; line-height: 2.0;">Daniel Khashabi </span>}} \n
Assistant Professor,  [https://www.cs.jhu.edu/ Department of Computer Science],
[https://jhu.edu/ Johns Hopkins University] \n
Office: Hackerman Hall 316B \n
Email: +danielk{{<img src="https://danielkhashabi.com/files/abababa.png" width="20" height="20" style="margin-bottom: -5px;">}}jhu.edu+ \n
{{<p style="margin-bottom:0.1cm;"></p>}}
Other affiliations:
 - [https://www.clsp.jhu.edu/ Center for Language and Speech Processing]
 - [https://ai.jhu.edu/ Data Science and AI Institute]
 - [https://iaa.jhu.edu/ Institute for Assured Autonomy]
 ~~~

{{<h2 id="themes">Research Themes</h2>}}

My research is motivated by understanding the computational foundations of /intelligent behavior/, often through the lens of /natural language/.
I am excited about [https://en.wikipedia.org/wiki/Intelligence_amplification /intelligence amplification/] â€” building computational models that would augment human experience in a mutually interdependent fashion.
The dominant majority of my research is aligned with /natural language processing/ (ACL, NAACL, EMNLP), /machine learning/  (ICLR, NeurIPS, ICML) and /artificial intelligence/ (AAAI, IJCAI).

# My goal is to explore the different ways in which we can develop and evaluate systems that understand and reason with (and about) natural language in different contexts.

Here are several themes I am interested in: 

- *General-purpose models:*
AlphaGo may be the world champion at Go, although it can't solve any other problem! How can we build models that generalize a broader scope of tasks, abilities, modalities, or environments?

- *Self-supervised representation learning:*
The AI literature has found powerful ways to build rich representations of the world by utilizing cheap signals available in the wild (web data, physical environment, etc.).
How can we make these algorithms more effective and efficient (in terms of data or computation cost)?
How can we make them robust to distributional drifts in data, e.g., low-data regimes or adversarial settings?
How can we scale them up to various modalities or forms of communication/interaction?

- *Reasoning and problem-solving:*  I view "reasoning" as the process of using "reasons" to explain or justify decisions.
 How can we enable machines to communicate via /reasons/, for a broad-ranging spectrum of tasks?
 How can we make this process "verifiable" or "explainable" to humans?
Can we build systems that can recourse upon a mistake?

- *Interaction, communication, and coordination:*  Can we engineer AI systems to effectively engage, interact, and communicate with humans and other AI systems for the purpose of, for example, coordination?

- *AI â†” humans:* /The ultimate goal of our work is to benefit humans!/ How should we engineer the interface between AI and machines?
What forms of interaction are most effective and meaningful for humans? How can we make AI systems more transparent and accountable to humans? Can we turn such transparency into a truly democratic oversight of systems, their algorithmic biases and mistakes? How should we think about personalizing AI systems to their users?



#  - Are system doing what they are supposed to do?
# The sad reality of our current AI technology is that our models, under the hood, often depend on shortcuts that are far from the human decision-making process. 
# Part of my research is dedicated to the ways in which we can bring out their unreliable correlations that could lead to catastrophic errors ([https://arxiv.org/abs/2004.02709 EMNLP 20]). 
# \n
# The tendency of our models to take advantage of problematic correlations could lead to algorithmic bias. No matter how small the implicit biases are in AI systems (say, a resume filtering system), in the long term they would contribute to inequitable #outcomes particularly for the marginalized communities of society. 
# This is an enormous challenge which requires the participation of policy-makers, social scientists, educators, and so on. Likewise, we, AI scientists, should be diligent about addressing problematic aspects of the same technologies we want to sell ([https://arxiv.org/abs/2010.02428 EMNLP 20]).


# AI systems should have the capacity to [http://danielkhashabi.com/files/2019_thesis_defense/danielk_dissertation_2019.pdf /reason/], in ways that are high-level and /not/ attached to individual tasks. 
# During my PhD, I worked towards this vision by creating symbolic systems that effectively utilize the knowledge of their context to solve a given problem ([http://danielkhashabi.com/files/2016_tableilp/2016_tableilp_ijcai.pdf IJCAI 16], [https://arxiv.org/abs/1906.03672 AAAI 18]). 
# More recently, I have pursued the /generalizability/ objective in terms of neural models that can solve a wider range of problems ([https://arxiv.org/abs/2005.00700 EMNLP 20]).

 # - What are more meaningful and effective ways to evaluate the progress of AI?
# We must actively rethink the evaluation of AI systems and  move the goalposts according to the latest developments. I persistently pursue this perspective in order to build more comprehensive challenges for our latest technology. 
# Instances of such vision are embodied in the design of our challenge datasets for language understanding ([https://www.aclweb.org/anthology/N18-1023/ NAACL 18], [https://arxiv.org/abs/1909.03065 EMNLP 19], [https://arxiv.org/abs/2101.02235 TACL 21]). 
# Along the same goal, we recently released [https://arxiv.org/abs/2101.06561 GENIEðŸ§ž], a human evaluation platform to fasciliatet comprehensive evaluation of text generation tasks. 

#  - How can AI benefit the common good?
# Whether we like it or not, technology will play an increasing role in our daily lives. 
# I tend to worry about the unintended consequences of technology in all areas of our lives. 
# These include, but are not limited to, algorithmic biases, misinformation, echo chambers and our growing socio-political divisions. 
# \n
# As a believer in technology, I like to see applications of AI that address the potential damages of new technologies. An instance of such mindset was embodied in our project that explored ways to organize ideas about controversial issues in ways that allows people to see alternative views and help them see alternative views to a discussion ([http://danielkhashabi.com/files/2019_perspectrum/2019_perspectrum_naacl.pdf NAACL 19]).  

# One of the highlights of my research career was the collaboration with a team of brilliant researchers who had [https://www.semanticscholar.org/paper/Combining-Retrieval%2C-Statistics%2C-and-Inference-to-Clark-Etzioni/478b4a5123bd5fda98bb35e6317d7f3555fec97d an ambitious goal of building a machine capable of answering elementary school science questions]. 
# Just a few years ago, the Aristo project [https://arxiv.org/abs/1909.01958 reached its milestone] by achieving over 90% in 8th grade science â€” [https://www.nytimes.com/2019/09/04/technology/artificial-intelligence-aristo-passed-test.html a milestone for AI and NLP].


~~~
 /If you are an undergraduate or masters student and would like to work on research with my group, please fill out [https://forms.gle/6AZg34JVkkZZieySA  this form]./\n
 /If you're seeking *postdoctoral* positions, consider applying for the [https://apply.interfolio.com/158121 DSAI fellowships]. (*deadline: Jan 06, 2025*)/
~~~



{{<h2 id="talks">Recent Talks</h2>}}

- 2024, University of Cambridge the Language Technology Lab seminar ([https://danielkhashabi.com//files/2024_cambridge_talk/2024-cambridge2.pdf slides])
- 2024, Oracle Labs ML seminar ([http://danielkhashabi.com/files/2024_oracle/2024-oracle.pdf slides])
- 2024, Tel Aviv NLP seminar ([http://danielkhashabi.com/files/2024_oracle/2024-TAU.pdf slides])
- 2024, Forum on ``Engineered AI Systems'' ([http://danielkhashabi.com/files/2024_NAE/2024-Khashabi_NAE.pdf slides])
- 2024, Keynote at ``Engineering for Professionals'' quarterly meeting ([http://danielkhashabi.com/files/2024_ep_conference/2024-EP-self-improve.pdf slides])
- 2024, Workshop on ``LLMs for Healthy Aging'' ([http://danielkhashabi.com/files/2023_aitc/2023-jhu-aitc.pdf slides])
- 2023, NYU ``Text-as-Data'' talk series ([http://danielkhashabi.com/files/2023_nyu/2023-nyu2-slides.pdf slides])
- 2023, JHU's Center for Language and Speech Technologies seminar ([https://www.youtube.com/watch?v=YuxmtgBk-Ls video])
- 2023, JHU's Electrical Engineering department seminars ([http://danielkhashabi.com/files/2023_ece_talk/2023-ece-jhu.pdf slides])
- 2023, Amazon ``Human in the Loop'' seminar
- 2023, JHU's Center for Health Security seminars ([http://danielkhashabi.com/files/2023_public_health/2023-jhu-public-health-split.pdf slides])
- 2023, UMD Computational Linguistics seminar ([http://danielkhashabi.com/files/2023_umd_talk/umd-talk-2023-split.pdf slides])
- 2023, Applied Physics Lab, Intelligent Systems Center seminars
- 2022, University of Tehran NLP seminar
- 2021, University of Glasgow IR seminar ([http://danielkhashabi.com/files/2021_glasgow/glasgow-talk-appril-2021.pptx slides])
- 2021, Johns Hopkins University ([http://danielkhashabi.com/files/2022_jhu_talk/jhu_talk-feb-2022-split.pdf slides])
- 2021, Google AI ([http://danielkhashabi.com/files/2021_google_talk/google-talk-march-2021.pdf slides])
- 2021, UCLA Big Data and ML seminar ([http://danielkhashabi.com/files/2021_ucla_talk/ucla-talk-march-2021.pptx slides])
- 2021, USC NLP seminar ([http://danielkhashabi.com/files/2021_usc/usc:isi-talk-march-2021.pptx slides])
- 2020, Tel Aviv University NLP seminar ([http://danielkhashabi.com/files/2020_unifiedqa/unifiedqa_tau_talk_split.pdf slides])
- 2019, [https://freuder.wordpress.com/pthg-19-the-third-workshop-on-progress-towards-the-holy-grail/The Workshop on Progress Towards the Holy Grail], Conference on Constraint Programming (CP), 2019. ([http://danielkhashabi.com/files/2019_talk_holy_grail_workshop/daniel-kh-holy-grail-workshop-2019.pdf slides])
- 2019, CMU LTI seminar ([http://danielkhashabi.com/files/2019_talk_indirect_supervision/danielkh_job_talk_small_supervision_split.pdf slides])
- 2018, NYU NLP seminar [https://docs.google.com/presentation/d/1KZxE0_LANeRYaShGP2cjvbRTaR2V7F8N36xQ8004r2o/edit?usp=sharing  Reasoning-Driven Question Answering].
- 2018, Stanford NLP seminar  ([https://docs.google.com/presentation/d/11QnDUYXB9q06HIyZxmHcb1BrwcdStjjHPNOC3AnlTsc/edit?usp=sharing slides])
- 2018, Mid-Atlantic Student Colloquium on Speech, Language and Learning ([https://docs.google.com/presentation/d/1KmGQfbeUitZOum4s6cPTqwuJJfTdxlQMeV_Go2_8Jz0/edit?usp=sharing slides])


{{<h2 id="teaching">Teaching</h2>}}
 - CS 601.471/671, NLP: Self-supervised Models: [https://self-supervised.cs.jhu.edu/sp2023/ Spring 2023], [https://self-supervised.cs.jhu.edu/sp2024/ Spring 2024], [https://self-supervised.cs.jhu.edu/sp2025/ Spring 2025]
 - CS 601.771, Advances in Self-supervised Models: [https://self-supervised.cs.jhu.edu/fa2022/ Fall 2022], [https://self-supervised.cs.jhu.edu/fa2024/ Fall 2024]


# - Guest Lecturer: CS446 (Machine Learning) @ UPenn, Spring 2018.
# - Guest Lecturer: CS446 (Machine Learning) @ UPenn, Fall 2018. # : [https://www.youtube.com/watch?v=9AbMmCrNeEQ Lecture 1] [https://www.youtube.com/watch?v=cznV5pRsv_A Lecture 2]
# - Guest Lecturer: CS446 (Machine Learning) @ UIUC, Spring 2016. # : [https://www.youtube.com/watch?v=oNOY2F2qcsk Lecture 1]
# - Guest Lecturer: CS446 (Machine Learning) @ UIUC, Fall 2015. # : [http://danielkhashabi.com/files/2015_cs446_machine_learning/ files] [https://www.youtube.com/watch?v=hYwGsu50Ee4&t=4s Lecture 1] [https://www.youtube.com/watch?v=Apxp1UfsTLk Lecture 2]
# - Teaching Assistant: [http://l2r.cs.uiuc.edu/~danr/Teaching/CS446-15/ CS446: Machine Learning],  Dan Roth, UIUC (Fall, 2015).
# - Teaching Assistant: [http://courses.engr.illinois.edu/cs473/fa2013/ CS473: Fundamental Algorithms], Jeff Erikson, UIUC (Fall, 2013).
# - Teaching Assistant: [http://courses.engr.illinois.edu/cs473/sp2013/ CS473: Fundamental Algorithms],  Sariel Har-Peled and Alexandra Kolla, UIUC (Spring, 2013).
# - Teaching Assistant: [http://ele.aut.ac.ir/~gholamrezamoradi/stat/ "Probability and Statistics"], Instructor: Dr. Gholamreza Moradi, AUT (Spring, 2012).
# - Teaching Assistant: "Digital Signal Processing", Instructor: Dr. Hamid Sheikhzadeh, AUT (Spring 2012).
# - Teaching Assistant: "C\+\+ II : Numerical C\+\+ Programming", Instructor: Dr.Bahram Taheri, Joint program with University of Birmingham and AUT (Fall, 2011). \n
# - Teaching Assistant: "Introduction to Computers and Programming(C\+\+)", Instructor: Dr.Hassan Taheri (Spring, 2011). \n


# == Past Projects and Resources
# - OpenEval, an open-source evaluation framework for AI applications, 2015, [http://danielkhashabi.com/files/2015_openeval/openeval_poster.pdf Poster]
# - [http://danielkhashabi.com/files/2015_knowledge_representation/knowledge_representation_writeup.pdf Knowledge Representation: How far we have come?], 2015, [http://danielkhashabi.com/files/2015_knowledge_representation/2015_knowledge_representation2.pdf PDF]
# - Modeling and control of ball-on-plate system, 2011, ([BallOnPlateControl.html details])
#- [http://danielkhashabi.com/files/2011_RVM_Facial_Feature/2011_internship_report.pdf Facial feature extraction and tracking], 2011
# - [http://danielkhashabi.com/files/2010_Wavelet/26.pdf Wavelet approximation for solving differential equations],  [http://danielkhashabi.com/files/2010_Wavelet/26-1.ppt slides]
# - [http://danielkhashabi.com/files/2010_GeneticCombinationalCircuits/23.pdf Combinational logic circuits using genetic algorithm], [GACombinatorial.html  details]
# - [http://cpp.aut.ac.ir/ C\+\+ programming and numerical methods], 2009 (in Persian) \n
# - Introduction to computers and programming using C\+\+, 2008, [http://danielkhashabi.com/files/2008_BLIF/CppTutorialBLIF.pdf PDF-Persian]
# - 2D Soccer Simulation, 2007, [http://danielkhashabi.com/files/Robocup/Hadaf06_TDP.pdf PDF-Persian]
#- Generating electricity with speed bumps, 2006, [http://danielkhashabi.com/files/2006_EPSEF/25.pdf  PDF-Persian]
#- Tom Mitchell's [http://danielkhashabi.com/files/MLbook/ "Machine Learning", translated by M. Nokhbeh Zaeeem] ({{<span lang="ko" xml:lang="ko">}}ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†-Ù…Ø­Ù…Ø¯ Ù†Ø®Ø¨Ù‡ Ø²Ø¹ÛŒÙ…{{</span>}})



#== Notes
# -  [learn.html "Learning Algorithms--or blend of computation and statistics", an outdated collection of notes]



{{<h2 id="publication">Publication</h2>}}

*Disclaimer:* This material is presented to ensure the timely dissemination of scholarly works. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms invoked by each author's copyright. \n
==== 

#include{publication.jemdoc}

 # {{<h2 id="lab">Intelligence Amplification Lab</h2>}}
 # PhD students:
 #   - [https://www.linkedin.com/in/adam-byerly-4ba0b0228/ Adam Byerly]
 #  - [https://ajwang34.github.io/ Andrew Wang] - co-advised with Mark Dredze and Nick Anderews
 # - [https://www.linkedin.com/in/candice-penelton-485b42102/ Candice Penelton] - co-advised

