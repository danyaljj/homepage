 % This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{pifont}
\usepackage{enumitem}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{tabularray}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{array}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{booktabs}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\newcounter{nTheorems}
% \theoremstyle{plain}
\newtheorem{theorem}[nTheorems]{Theorem}
\newtheorem{corollary}[nTheorems]{Corollary}
\newtheorem{conjecture}[nTheorems]{Conjecture}
\newtheorem{lemma}[nTheorems]{Lemma}
\newtheorem{proposition}[nTheorems]{Proposition}
\newtheorem{protocol}[nTheorems]{Protocol}
\newtheorem{claim}[nTheorems]{Claim}
\newtheorem{fact}[nTheorems]{Fact}

\newcounter{nDefinitions}
% \theoremstyle{definition}
\newtheorem{definition}[nDefinitions]{Definition}
\newtheorem{problem}[nDefinitions]{Problem}
\newtheorem{intuition}[nDefinitions]{Intuition}
\newtheorem{idea}[nDefinitions]{Idea}
\newtheorem{exercise}[nDefinitions]{Exercise}
\newtheorem{remark}[nDefinitions]{Remark}
\newtheorem{example}{Example}

\newcounter{nAssumptions}
% \theoremstyle{definition}
\newtheorem{assumption}[nAssumptions]{Assumption}

\usepackage[most]{tcolorbox}
\AtBeginEnvironment{tcolorbox}{\small}


\title{Instructions for *ACL Proceedings}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

\newcommand{\fzx}[1]{\textbf{\color{blue}[{\bf Zhouxiang}: #1]}}
\newcommand{\daniel}[1]{{\color{purple}[{\bf DK}: #1]}}
\newcommand{\aayush}[1]{{\color{green}[{\bf Aayush}: #1]}}
\newcommand{\muhan}[1]{{\color{green}[{\bf Muhan}: #1]}}



\title{ICL Robustnes to Vocab Shuffling}
\title{ICL can reverse-engineer (or, recover) bijective word shufflings }
\title{LLMs Can In-context Recover Novel Bijections Word Shufflings}
\title{LLMs Can In-context Learn novel Bijective Word Shuffling}
\title{LLM Caesar Cipher: In-Context Learning Recovers Bijective Word Shuffling}
\title{Quantifying ``Learning'' in In-Context Learning via Language Cipher Problems}
\title{Quantifying ``Learning'' in In-Context Learning: \\  A Case Study on Language Cipher Problem}
\title{How much ``Learning'' Happens In-Context? \\  A Case Study on Language Cipher Problem}
\title{Measuring ``Learning'' in In-Context Learning via Language Cipher Problems}
\title{How much ``Learning'' is in In-Context Learning? \\  A Case Study on Language Cipher Problem}
\title{How much does In-Context Learning ``Learn''? \\  A Case Study on Language Cipher Problem}
\title{Quantifying ``Learning'' in In-Context Learning \\  via Language Ciphers}
\title{\name: Quantifying ``Learning'' in In-Context Learning \\ via Substitution Ciphers}



% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

\setlength\fboxsep{1pt}

\def\mystrut(#1,#2){\vrule height #1pt depth #2pt width 0pt}

\definecolor{purple}{rgb}{0.5,0,1}
\definecolor{dcyan}{rgb}{0.2,0.6,0.5}
\definecolor{DarkGreen}{RGB}{51,140,0}
\definecolor{light-gray}{gray}{0.95} % used in table

\definecolor{darkgreen}{RGB}{0,140,0}
\definecolor{darkred}{RGB}{200,0,0}
\definecolor{DarkRed}{RGB}{200,0,0}
\definecolor{lightgreen}{RGB}{189,252,192}
\definecolor{lightred}{RGB}{255,205,212}
\definecolor{lightyellow}{RGB}{255,240,160}
\definecolor{lightblue}{RGB}{195,221,255}
\definecolor{lightpurple}{RGB}{232,209,255}

\newcommand{\redtext}[1]{\colorbox{lightred}{\mystrut(.5, .5) #1}}
\newcommand{\greentext}[1]{\colorbox{lightgreen}{\mystrut(.5, .5) #1}}
\newcommand{\yellowtext}[1]{\colorbox{lightyellow}{\mystrut(.5, .5) #1}}
\newcommand{\bluetext}[1]{\colorbox{lightblue}{\mystrut(.5, .5) #1}}
\newcommand{\purpletext}[1]{\colorbox{lightpurple}{\mystrut(.5, 0.5) #1}}

\newcommand{\name}{\textsc{ICL Ciphers}}
\newcommand{\gemma}{Gemma 2 }
\newcommand{\llama}{Llama 3.1 }
\newcommand{\qwen}{QWen 2.5 }
\newcommand{\olmo}{OLMo }
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (remember learned patterns from pre-training) and task learning (inference-time ``learning" from demonstrations). However, disentangling these the two modes remains a challenging goal.

We introduce \name, a class of task reformulations based on \emph{substitution ciphers} borrowed from classic cryptography.
In this approach, a subset of tokens in the in-context inputs are  substituted with other (irrelevant)
% random
% \aayush{is random the correct word here?}
tokens, rendering English sentences less comprehensible to human eye.
However, by design, \emph{there is a latent, fixed pattern to this substitution, making it reversible}.
This bijective (reversible) cipher ensures that
% This ensures that
the task remains a well-defined task in some abstract sense, despite the transformations.
It is a curious question if LLMs are capable of solving \name{} with a bijective map, which requires deciphering the latent cipher.
% and hence provide a novel approach to quantify ``learning'' in ICL.
% which essentially would require deciphering the latent  cipher, providing a novel approach to quantify ``learning'' in ICL.

We show that LLMs are better at solving \name{} with bijective maps than the non-bijective (irreversible) baseline,
providing a novel approach to quantify ``learning'' in ICL.
% \aayush{how about we say, LLMs are able to decipher well-formed ciphers and }
% However, this gap remains small across the board (e.g., datasets, models, scale).
While this gap is small, it is consistent across the board on four datasets and four models families.
Finally, we examine LLMs' internal representations and identify evidence in their ability to decode the ciphered inputs.

\begin{comment}
% \daniel{I think we need a motivating sentence here: how should we isolate task ``learning'' from ``recall'? Then tie it  towhar we do. }
% and  provide a novel way of disentangling ``learning'' vs ``retrieval'' in ICL.
% study ICL in LLMs using token-level substitution ciphers, which single out the task-learning mode.
% The first step is to define a cipher \emph{key}, a random, bijective map over the LLM vocabulary.
% where all the tokens have a one-to-one correspondence with those in the original vocab.

% Intuitively, if LLMs are truely capable of learning
% Then we use this  substitute tokens in the original input text according to the bijective mapping.

% \daniel{I think we need a motivating sentence here: how should we isolate task ``learning'' from ``recall'? Then tie it  towhar we do. }
Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (when the model uses demos to retrieve an underlying capability learnt during pre-training) and task learning (when the model actually uses demos to ``learn" the task during inference). However, disentangle these the two modes remains a challenging goal.

In this work, we study ICL in LLMs using token-level substitution ciphers, which single out the task-learning mode.
\fzx{explained the substitution cipher}
The first step of substitution cipher is to create a shuffled vocab through bijectively shuffling, where all the tokens have a one-to-one correspondence with those in the original vocab.
We then substitute tokens in the original input text according to the bijective mapping.
The performance on three downstream tasks \fzx{or four datasets} shows LLMs' ability to solve substitution ciphers.
We also adopt Logit Lens to reveal LLMs' internal representations for the ciphers. Our work quantifies “learning” in ICL by introducing the substitution cipher task, indicating ICL's task-learning mode.
\end{comment}
\begin{comment}
Large Language Models (LLMs) have demonstrated impressive performance via in-context learning (ICL).
% However, recent evidence shows that ICL could be sensitive to a variety of factors (such as word order).
However, there is still not enough understanding of what can be learned from the ICL paradigm.
In this work, we explore the capability of ICL of solving token-level Caesar cipher.
Specifically, we shuffle the vocabulary in a bijective way and map the input text accordingly.
We conduct extensive experiments on multiple tasks, datasets and models, the results of which show that:
(1) model performance drops as shuffling rate grows, but always stays considerably better than random;
(2) model performance improves as the number of demos increases.
We then use a novel probing mechanism to reveal to model's evolving understanding for shuffled tokens, which is driven by the demos.
Our work has shown LLMs are able to solve the token-level Caesar cipher and capture the change of token's meaning through similar demos and context.
\end{comment}
\end{abstract}

\section{Introduction}
\label{sec:intro}

% \daniel{
% For future: we should not use the term "random baseline", for several reasons:
%  - We should not call it a "baseline": As we can see from the reviews, once people/reviewers hear "baseline" they generally expect another "baseline". It's common to have multiple "baseline"
%  - We should not call it "random": even bijective shuffling contains some randomization in it.\\
% % My suggested term is "uninformed" or "uninformative" shuffling.
% }



In-Context Learning (ICL) is an emergent behavior in Large Language Models (LLMs) that allows them to identify patterns in demonstrations given as prompts and apply these patterns to similar tasks~\citep{brown2020language}.
This intriguing inference-time learning ability has spurred numerous studies to better understand its dynamics. Despite recent efforts~\cite{xie2021explanation,min2022rethinking,srivastava2023beyond,shin2022effect,razeghi2022impact,von2023transformers,shen2024icl_vs_gd}, the literature's understanding of the functional aspects of ICL remains elusive and contentious.
% For instance, there is an ongoing debate about merits and limitations of viewing ICL as an implicit optimization problem~\citep{von2023transformers,shen2024icl_vs_gd}. Additionally, some research has indicated that LLMs may be indifferent to the order of labels or input frequencies~\citep{min2022rethinking, razeghi2022impact} which are not explained by any existing formal framework.



% Despite many recent efforts~\cite{srivastava2023beyond} \daniel{CITE} our understanding of the functional behavior of In-Context Learning (ICL) still remains mysterious and debated.
% For example, there has been extensive debates on merits and limitations of interpreting ICL as an implicit optimization problem~\citep{von2023transformers,shen2024icl_vs_gd}.
% Other works have shown that LLMs can be insensitive to label ordering, which undermines the learning hypothesis~\citep{min2022rethinking, pan2023incontext}.
% Some studies claim that ICL works as implicit optimization~\citep{akyurek2022learning, von2023transformers, dai-etal-2022-dialog}, but they use a tangential setting where transformers are trained to explicitly perform ICL~\citep{shen2024icl_vs_gd}.
% In fact, the most recent theories \cite{lee} suggest that ICL has a dual form with both task retrieval (or recall from pre-training data) and task learning.

\begin{figure}[t]
\includegraphics[width=19cm,trim=3.1cm 2.9cm 2cm 0.8cm,clip=true]{fig/teaser1.pdf}
\caption{
An example of \name, a cryptographic task reformulations framework where a subset of tokens are ciphered (replaced with other tokens in the lexicon) via a bijective mapping (e.g., each instance of ``school''  is replaced with ``apple''.)
Since this cipher is a bijection, one can recover the original format of the ICL instance, ensuring the well-defined task upon the transformations.
% We consider a variant of ICL problems  where a subset of tokens are encrypted through a bijective map.
% For example, every instance of ``school'' is replaced ``apple''.
% Inevitably, this turns
}
\label{fig:pipeline}
\end{figure}

Most pertinent to our study, \citet{pan2023incontext,lin2024dual,wang2024investigating} propose ICL's dual behavior: \textbf{\emph{task retrieval}} (TR), which involves recalling a previously encountered task from pre-training data through its demonstrations, and \textbf{\emph{task learning}} (TL), which refers to the ability to grasp new input-label mappings that were \emph{not} seen during pre-training. Although these two mechanisms are not necessarily separate in practice, examining them independently may help researchers better understand their strengths and limitations. Specifically, \citet{pan2023incontext} measure TL by assessing task performance when labels are substituted with abstract symbols (such as numbers or letters) that have never co-occurred with the inputs during pre-training. However, it remains unclear whether this label mapping is sufficient to ensure pure TL (i.e., no partial influence of TR).
% Many tasks that are human-readable may have inherent priors embedded in the pre-training datasets.
It is conceivable that LLMs could still use the human-readable inputs and prompt structure to deduce the task, thereby performing implicit task retrieval.
Fundamentally differentiating TR and TL is hard because it is hard to know whether the learning signal comes from the in-context examples or pretraining data even after modifying input-label mapping suggested in prior work which leaves the inputs intact and leaves the door open for LLM to exploit human-readable inputs and prompt structure to deduce the task, thereby performing implicit task retrieval.
This consideration motivates the exploration of alternative approaches for quantifying task learning.

% Most relevant to the focus of our work, \cite{pan2023incontext,lin2024dual,wang2024investigating} suggest that ICL operates under a dual form: \textbf{\emph{task retrieval}} (TR) or recalling (through its demonstrations) an already seen task  from pre-training data  and task \textbf{\emph{task learning}} (TL) or the ability to capture new input-label mappings unseen in pre-training.
% While in practice, these two mechanisms are not necessarily disjoint, studying them in isolation may allow researchers to better understand their strengths/limits.
% Specifically, \citet{pan2023incontext} quantifies \emph{task learning} by defining task performance where the labels are replaced with abstract symbols
% (e.g., numbers or letters) that never co-occurred with the inputs in pre-training.
% However, it is not unclear whether this label remapping is enough for ensuring pure \emph{task learning} (i.e., no \emph{task retrieval}).
% Most interpretable tasks may have priors already built in the pretraining datasets.
% Arguably, LLMs may still be able to inputs and prompt structure to infer the task and hence, do an implicit task retrieval.
% This motivates, exploring alternative formulations for quantifying \emph{task learning}.


% \paragraph{Dual modes of ICL} \citet{pan2023incontext} demonstrated how In-Context Learning shows a dual behavior of \textit{Task Recognition} (TR) and \textit{Task Learning} (TL). They use random and abstract labels on classification tasks to measure TR and TL respectively. Random labelling induces noise in the data so any performance that
% we achieve after this intervention would be due to TR. However, it is not clear if making labels abstract is enough to suppress TR. The model might still use the inputs and prompt structure to use TR. In this work, we induce abstraction (via substitution ciphers) in the task inputs instead. We argue that this results in more effective suppresion of TR as the model needs to decipher the inputs before figuring out the task at hand.



% We posit that the two mechanisms occur under separate conditions, as recognizing an already
% learned task is easier than learning a new mapping.
% Models are able to perform TR at a small scale,
% but this ability does not drastically improve with
% increasing model sizes and demonstrations; on the
% other hand, TL improves significantly when model
% sizes and numbers of demonstrations increase.\aayush{Should we preface this by mentioning why the disentanglement is necessary? answer: because finding tasks that do not have pretraining priors is hard. sidenote: Our problem setup with say the SST2 problem, may still use some TR.} To
% show the above phenomenon, we disentangle TR and TL through label space manipulation, including three different setups (examples in Figure 1):


% However, it is hard to find tasks that can be explicitly used to study the task learning capacity of LLMs. This is because most interpretable tasks may have priors already built in the pretraining datasets. Therefore, it is hard to disentangle task retrieval and task learning in real LLMs. To alleviate this problem and quantify pure learning in LLMs, we study the learning behavior of LLMs with bijection ciphers.


% designed to understand and measure \emph{task learning} within ICL.

In this study, we introduce \name,
% a cryptographic framework that uses substitution ciphers to encode task inputs.
a class of prompt reformulations based on \emph{substitution ciphers} borrowed from classic cryptography, applied to task \textit{inputs}.
For example, in a sentiment classification task where sentences are assigned to target classes, we apply bijective shuffling to the LLM's original vocabulary, ensuring a one-to-one correspondence between tokens in the shuffled and original vocabularies. This random bijective shuffling is done before the experiments and remains constant throughout. We then replace tokens in the input text with their corresponding tokens based on this mapping (e.g., every instance of ``love'' is replaced with ``today''), as shown in \autoref{fig:pipeline}.




% In this study, we introduce \name, an cryptographic framework that employs substitution ciphers for encoding task inputs.
% Take, for instance, the sentiment classification task, where a set of sentences needs to be assigned to their respective target classes.
% As illustrated in \autoref{fig:pipeline}, we first apply bijective shuffling to the original vocabulary of the LLM, ensuring that all the tokens in the shuffled vocabulary have a one-to-one correspondence with the tokens in the original vocabulary.
% This bijective shuffling is conducted randomly before the experiments and remains constant throughout.
% We then substitute tokens in the input text with their corresponding tokens
% according to the bijective mapping.
% (e.g., every instance of ``love'' is replaced with ``today'').

% (or correspondence)
% between two vocabularies.
% The encryption process by which our inputs are transformed is a \emph{substitution cipher} which each token is replaced with another token, based on a fixed \emph{key}.
% As illustrated in Fig.~\ref{fig:pipeline} (bottom), we substitute certain tokens in the input with different tokens (e.g., every instance of ``love'' is replaced with ``today'').
% Crucially, the mapping of these words adheres to a predetermined (yet random) mapping, established through a bijective shuffling function applied to the items in the LLM vocabulary (Fig.~\ref{fig:pipeline}; top).
% This bijective function is generated randomly before the experiments and remains constant throughout.
% This cipher was widely used historically due to its simplicity~\cite{shannon1949communication}, but no longer used due to its vulnerability.
% However, they remain important for teaching basic cryptography concepts and are sometimes used in puzzles or recreational cryptography.
% \fzx{The description for encryption process looks kinda confusing to me. Wrote a modified version in comment}



% In this work, we induce ICL Ciphers, anencoding (abstraction) for understanding and quantifying \emph{task learning} in ICL.
% Our approach uses substitution ciphers of task inputs.
% Consider the sentiment classification task where collection of sentences ought to be mapped to their target classes.
% As the example in Fig.~\ref{fig:pipeline} (bottom) shows,
% we replace a subset of the tokens in the input with other words (e.g., all mentions of ``love`` is replaced with ``today'').
% Importantly, the mapping of these works follows a pre-determined (but random) mapping defined based on a bijective shuffling function defined on  the items in the LLM lexicon (Fig.~\ref{fig:pipeline}; top).
% This bijective function is generated randomly prior the experiments and fixed throughout.


% Historically popular for its simplicity, hwow
% In the modern era, simple substitution ciphers hold limited practical use for secure communication due to their vulnerability to cryptographic attacks.

% including by Julius Caesar (the Caesar cipher), but it is vulnerable to frequency analysis, making it easily crackable with modern techniques.

% Historically popular for its simplicity, as used by Julius Caesar, this cipher is susceptible to frequency analysis and easily broken with modern methods. Today, simple substitution ciphers are largely impractical for secure communication due to their vulnerability to cryptographic attacks, but they are valuable for teaching basic cryptography concepts and are occasionally used in puzzles or recreational cryptography.

The outcome of substitution ciphers is generally not easily interpretable by humans (see Fig.\ref{fig:pipeline} for examples), resembling a random shuffling of words. However, since ICL ciphers are \emph{reversible}, the original tasks can be reconstructed from the encoded version, ensuring that the task, although not easily understood by human eyes, still represents a valid task.
This lack of interpretability is a design feature (rather than a flaw) here as it greatly reduces the likelihood that our prompts have been encountered in the pre-training data.
As a results, our working hypothesis is that any gains above the random (non-bijective shuffles should be indicative of TL (as opposed to TR) within ICL.

% This lack of interpretability is an intentional design feature, not a flaw, as it greatly reduces the likelihood that our prompts have been encountered in the pre-training data.


% The result of substitution ciphers are not necessarily understandable to human eye (see \daniel{TODO} for examples), almost like a random word shuffling.
% However, given that ICL ciphers are \emph{reversible},
% it entails that the original tasks can be recovered from the resulting encoding, and hence, the task (despite not being understandable) encode a valid task.


% Our approach: A novel task, unlikely to be seen in pretraining.

% Having a bijective map ensures that the encoded task is \emph{reversible}---it can be recovered from the resulting encoding.
% As a result, even though the ciphers make the task not understable to human eye

% As show in Fig.~\ref{fig:pipeline} our pipeline involves building a bijective shuffling of the items in the LLM lexicon.

% These learnable functions can not have a prior in the pre-training dataset as they are randomly generated.
% We find that:
% \begin{itemize}
    % \item Models are able to learn these random bijections and begin using them ...
    % \item We quantify the ``learning" and it increases proportionally to the number of demos shown ...
    % \item ...
% \end{itemize}




We evaluate ICL ciphers using 4$\times$ pre-trained models across 4$\times$ well-known benchmarks and  different range few-shot numbers demonstrations. Our empirical results demonstrate that ICL achieves better-than-random performance on ciphered tasks (\S\ref{results:llm:solve:it}).
For example, on the bijective ciphered HellaSwag, Llama3.1 (8B) averages 3.6\% higher accuracy than random shuffling, across various  demonstration counts (\autoref{tab:llama8b_infor_sample}).
% \daniel{TODO for later: a concrete statement about the gaps}
This suggests that LLMs can learn and decode these random bijections, enabling them to solve ICL Ciphers. Furthermore, we provide additional results
% measure the extent of this ``learning,''
with the number of demonstrations, model scale, shuffling rate. Finally, we perform an interpretability analysis (\S\ref{sec:probing})  which reveals promising, albeit weak, trends in their ability to decode the ciphered inputs.

% \daniel{Add one sentence about the finding of the probing}




% Introduce  pipeline for numeric results
% We evaluate ICL ciphers with 3 pre-training models and 4 well-known tasks.
% Empirically we show that ICL shows better than random performance on ciphered tasks (\S\ref{}).
% \daniel{TODO for later: a concrete statement about the gaps}
% This indicates that LLMs are able to learn (and recover) these random bijections and begin using them solve ICL Ciphers.
% We quantify the ``learning" and it increases proportionally to the number of demos shown \daniel{TODO}
% Last but not least, we conduct interpretability analysis to better understand our findings (\S\ref{}).
% \daniel{Add one sentence about the finding of the probing}

Unlike previous work by \cite{pan2023incontext,wang2024investigating} that intervenes in task outputs through label shuffling, our approach modifies task inputs. This creates instances less likely to have been encountered in pre-training data, offering an alternative TL indicator by necessitating the LLM to decode ciphers as part of task solving. These perspectives can be seen as complementary, each assessing different aspects of ``learning'' in ICL.


% Unlike previous work by \cite{pan2023incontext,wang2024investigating} that intervenes in task \emph{outputs} through label shuffling, our approach modifies task \emph{inputs}. This results in instances less likely to have been seen in pre-training data, providing an alternative indicator to  TL by requiring the LLM to decode ciphers as a necessary part of task solving. These two perspectives can be viewed as complementary, each measuring different aspects of ``learning'' in ICL.


% Unlike the previous work by \cite{pan2023incontext,wang2024investigating}, which focuses on intervening in task \emph{outputs} (through label shuffling) to assess TL, our approach intervenes in the task \emph{inputs} which  results in instances that are less likely to have been encountered in the pre-training data, thus serving as a better indicator of TL---requiring the LLM to decode the ciphers as a necessity  component of task solving.
% Alternatively, these two perspectives can be seen as complementary, each quantifying different aspects of learning within ICL.

% In contrast to the prior work by \cite{pan2023incontext,wang2024investigating} where they apply an intervention on task \emph{outputs} (label shuffling) to measure TL, we apply our intervention on the task \emph{inputs} instead.
% Intuitively, our intervention leads to tasks that have a lower chance of being observed in the pretraining data and hence, a better indicator of TL---LLM needs to decipher the inputs before figuring out the task at hand.
% On a different account, one can view these two perespectives complementary which quanitfy different aspects of learning in ICL.




% We argue that this results in more effective suppresion of TR as the model needs to decipher the inputs before figuring out the task at hand.


In conclusion, we propose an alternative method for quantifying ``learning'' in ICL through the use of substitution ciphers. We establish a framework for evaluating the performance of ICL within this experimental context. Our findings demonstrate evidence of task learning in ICL, both in terms of downstream performance and through probing analysis.
As far as we know, this is the first work to propose such cryptographic approaches for quantifying genuine ``learning'' in in-context demonstrations.
We hope these insights inspire future research aimed at gaining a deeper understanding of the emergence of ICL in LLMs.


% In summary, we define an alternative approach to quantifying ``learning'' in ICL using substitution ciphers.
% We develop the pipeline for quantifying performance of ICL under this experimental setting.
% Under these settings, we show the evidence of task learning with ICL both in terms of downstream performance and also via probing analysis. We hope these findings motivate future work in on better understanding on the emergence of ICL in LLMs.

% \begin{itemize}
%     \item Explore the nature/ propose a new task
%     \item show it can solve the ICL cipher
%     \item proving with probing experiments
% \end{itemize}



\section{Related Work}
\label{sec:related_work}

\paragraph{Dual operating modes of ICL:}
\citet{min2022rethinking} showed the disconnect between ``learning'' and the content of in-context demonstrations (lack of task ``learning''). This motivated follow works to identify two primary modes of operation for In-Context Learning (ICL): \emph{task retrieval} (TR), which involves recalling patterns previously encountered in pre-training data, and \emph{task learning} (TL), which involves learning new patterns on-the-fly that were not seen during pre-training. Some studies emphasize TR by exploring the factual recall capabilities of ICL~\citep{sun2022recitation, golchin2024memorization, han2023understanding,zhao2023context,reddy2023mechanistic,dankers2024generalisation}, providing insights into how LLMs memorize pre-training data, thus facilitating TR. Other studies~\cite{lin2024dual,song2024out,nafar2024learning,anand2024dual} focus on simplified datasets (e.g., linear regression) or architectures (e.g., shallow transformers), which differ from our focus. Additionally, \citet{pan2023incontext,wang2024investigating} have attempted to separate TR and TL through \emph{output} intervention by replacing labels with abstract symbols like numbers or letters. However, it remains uncertain whether using abstract labels effectively eliminates the influence of TR in ICL. Many human-readable tasks may have inherent priors embedded in the pre-training datasets, suggesting that LLMs might still use inputs and prompt structures to infer the task, thereby engaging in implicit task retrieval. Our approach proposes an alternative method for quantifying TL by intervening in the \emph{input} space.

% Several prior work discuss two operating models for ICL: \emph{task retrieval} (TR), for recalling a previously-encountered patterns from pre-training data and \emph{task learning} (TL) for on-the-fly learning of new patterns were \emph{not} seen during pre-training.
% A subset of works focus on TR through factual recall capabilities through ICL~\citep{sun2022recitation, golchin2024memorization, han2023understanding,zhao2023context,reddy2023mechanistic,dankers2024generalisation}, which provide more insights about LLM's memorization of their pre-training data which gives rise to TR.
% Another subset of these prior works ~\cite{lin2024dual,song2024out,nafar2024learning,anand2024dual} focus on simplified datasets (e.g., linear regression) or architecture (e.g., shallow transformers) which are different than our focus.
% Last but not least, \citet{pan2023incontext,wang2024investigating} disentangled TR and TL through \emph{output} intervention: labels are substituted with abstract symbols such as numbers or letters.
% However, it is unclear if using abstract labels removes the effect of TR in ICL.
% Many tasks that are human readable may have inherent priors embedded in the pre-training datasets. It is conceivable that LLMs could still use inputs and prompt structure to deduce the task, thereby performing implicit task retrieval.
% Our proposal offers an alternative construction for
% approaches for quantifying TL by interventions in the \emph{input} space.

% Specifically, \citet{pan2023incontext} measure TL by assessing task performance when labels are substituted with abstract symbols (such as numbers or letters) that have never co-occurred with the inputs during pre-training.
% However, it remains unclear whether this label mapping is sufficient to ensure pure TL (i.e., no partial influence of TR).


% \citet{song2024out} generated ICL demonstrations according to hidden (symbolic) rules.
% Arguably, these factual recall capabilities give rise to the TR capabilities of ICL.
% 's ability to recover memorized information (such as copyrighted content)
% \citet{yuan2024towards} propose a comprehensive benchmark for measuring these factual recall of LLMs.

% LLMs have been shown to memorize training data~\citep{}. Some works \citep{sun2022recitation, golchin2024memorization, han2023understanding} use ICL to recall information memorized during pre-training (such as copyrighted content). \citet{yuan2024towards} propose a comprehensive benchmark for measuring these factual recall of LLMs. Arguably, these factual recall capabilities give rise to the TR capabilities of ICL.
% \citet{pan2023incontext} demonstrated how LLMs show dual behavior when performing ICL and that TR plays a crucial role in ICL in small as well as big models. With larger models, the ability of LLMs to perform TL increases. \cite{nafar2024learning} studied the balance between learning vs retrieval for ICL for the special setting of linear regression. \citet{lin2024dual} have theorized how this dual behavior emerges in transformer models, but this work still suffers from an unrealistic study setup~\citep{shen2024icl_vs_gd}.

% Although these two mechanisms are not necessarily separate in practice, examining them independently may help researchers better understand their strengths and limitations.





% \cite{bertsch2024context}


% \paragraph{Dual modes of ICL} \citet{pan2023incontext} demonstrated how In-Context Learning shows a dual behavior of \textit{Task Recognition} (TR) and \textit{Task Learning} (TL). They use random and abstract labels on classification tasks to measure TR and TL respectively. Random labelling induces noise in the data so any performance that
% we achieve after this intervention would be due to TR. However, it is not clear if making labels abstract is enough to suppress TR. The model might still use the inputs and prompt structure to use TR. In this work, we induce abstraction (via substitution ciphers) in the task inputs instead. We argue that this results in more effective suppresion of TR as the model needs to decipher the inputs before figuring out the task at hand.

% \citet{lin2024dual} then theorized how this dual behavior emerges in transformer models.

% Ever since In-Context Learning was discovered~\citep{brown2020language}, it has been studied from multiple perspectives. Many works made useful empirical observations about ICL~\citep{zhao2021calibrate,min2022rethinking,mishra2022reframing,han2023understanding,wang2023selfinstruct}. For example, \citet{srivastava2023beyond} benchmarked ICL under multiple tasks and models.
% \citet{perez2021true,Lu2022FantasticallyOP} showed the sensitivity of ICL to the choice of demonstrations and their orderings.
% \citet{shin2022effect,razeghi2022impact} showed the sensitivity of ICL performance to the frequency and size of the relevant pre-training corpus. \citet{wei2022chain} used chain-of-thoughts to extract more performance out of language models.

% Another line of work tries to understand the origin of ICL in LLMs. \citet{xie2021explanation} explain ICL as implicit Bayesian inference, which maps a ICL demonstrations to a latent concept (task) learned via pre-training.  \citet{hahn2023theory} posited that compositional structure in natural language gives rise to emergent in-context learning. Other works~\citep{chan2022data, ...} theorize other distributional properties in the pre-training data, that may give rise to ICL.

% A more recent line of study aims to understand functional nature of ICL, i.e. how does ICL actually work? ICL has been compared with implicit optimization (specifically gradient descent)~\citep{garg2022can,zhang2023trained,dai2022can,akyurek2022learning,von2023transformers,li2023closeness}. This line of work claims that Transformers can meta-learn to perform optimization of internal models given a set of demonstrations. However, this does not explain the task retrieval capabilities of LLMs.

% %% memorization
% Some works \citep{sun2022recitation, golchin2024memorization} use ICL to recall information memorized during pre-training (such as copyrighted content). \citet{yuan2024towards} propose a comprehensive benchmark for measuring these factual recall of LLMs.
% % In the same vein \citet{} generates (``recites'') it memorized information during pre-training that may be useful for solving various knowledge-driven tasks.

% % Solving unnatural problems
% \aayush{more related to ours}\citet{yuan2023gpt} show that GPT-4 is able to resolve/recover non-natural languages-ciphers that may pose risks to LLMs.


% %% LMs doing chess

% % %% LLM factual recall
% % \cite{yuan2024towards} proposes a comprehensive benchnmarking for measuring factual recall of LLMs


% \citet{song2024out} examines OOD generalization in settings where ICL demonstrations are generated according to hidden (symbolic) rules and LMs are required
% to in-context infer the hidden rules behind input prompts



% \cite{nafar2024learning} studies the balance between learning vs retrieval for ICL for the special setting of linear regression, which as it has been argued in prior work~\cite{shen2024icl_vs_gd} it is somewhat artificial and unclear whether it quite aligns with the emergent in-context learning in LLMs.

% \cite{vacareanu2024words}

% \cite{zhao2023context}
% \cite{mao2024data}

% \cite{mueller2024context} ICL for syntax


% \cite{han2023understanding} ICL from the perspective of memorization


% \cite{bertsch2024context}

% \cite{reddy2023mechanistic}

% \cite{anand2024dual}

% \cite{dankers2024generalisation}

% \cite{sia2024does}

% \daniel{Copied from different paper; need to revise/expand}

% \paragraph{Functional explanations.}
% Many works offer functional explanations of ICL~\citep{liu2022Transformers,olsson2022context,schlag2021linear}.
% % For example, ~\cite{olsson2022context} hypothesizes the existence of special circuits inside Transformer models, which is responsible for in-context learning.
% % \cite{schlag2021linear} showed an equivalence between linearized self-attention Transformers and fast weight programmers~\citep{schmidhuber1992learning}.
% Among these, explanations via GD~\cite{garg2022can,zhang2023trained,ahn2023Transformers} are most pertinent to our work. Notably,
% \citet{akyurek2022learning} showed that Transformers can implement learning algorithms (gradient descent or closed-form OLS) for linear regression problems and empirically showed that the optimality of algorithms implemented experience a \textit{phase shift} with increasing model size. \citet{raventos2023pretraining} discovered similar results about algorithm discovery and phase shifts with increasing task diversity. \citet{dai2022can} similarly showed a dual between attention layers and linear layers optimized using gradient descent. \citet{li2023closeness} showed such an equivalence on softmax regression tasks.
% % \cite{garg2022can,zhang2023trained} then show that using an ICL objective, Transformers can learn not only linear function classes in-domain, but also out-of-domain and even sparse linear functions, decision trees and two layered ReLU neural networks.
% % \cite{ahn2023Transformers} show that Transformers trained on random instances of linear regression also develop the ability to perform GD on linear regression tasks.
% Finally, \citet{von2023transformers} showed a similar construction with a simpler Linear Self-Attention Transformer, claiming that Transformers learn in-context using gradient descent on linear regression problems. Notably, \citet{akyurek2022learning} found this GD behavior applicable only in small models, with bigger models exhibiting Bayes optimal learning behavior (like Ordinary Least Squares for linear regression). In contrast, \citet{von2023transformers} claimed that bigger Transformers also implement GD with added data transformations.
% % In this work, we argue how the equivalence between ICL and GD is does not extend to real-world models.
% % Furthermore,~\cite{akyurek2022learning,garg2022can} show that in-context trained Transformers can mimic implementing linear regression algorithms (e.g., gradient descent) with detailed derivation.
% % Our work contributes to and complements this line of work by (1) finding that ICL is not always close to CPT and explaining when ICL is very similar to CPT. (2) we try to explain ICL based on the real LLMs instead of Transformers pretrained on linear functions~\citep{akyurek2022learning,garg2022can,giannou2023looped}.


% \paragraph{Distributional explanations.} This body of work explains ICL via distributional frameworks and the relevant properties of LLMs~\citep{xie2021explanation,wies2023learnability}.
% \citet{xie2021explanation} explained ICL as implicit Bayesian inference, which implicitly maps a given set of demonstrations to an appropriate latent concept (task) learned via pretraining on a massive unsupervised corpus.
% % which can emerge by pretraining on documents having long-range coherence.
% Similarly, \citet{hahn2023theory} theorized that natural language pretraining data consists of compositional structure, which leads to the emergent ability of in-context learning, while \citet{chan2022data} showed that this might be because of distributional properties of the training distribution (like burstiness).
% These are all reasonable explanations of how ICL works, although they are somewhat tangential to the focus of this study.



% \paragraph{Empirical studies.}
% Various empirical works study ICL under various settings~\citep{brown2020language,zhao2021calibrate,min2022rethinking,mishra2022reframing,han2023understanding,wang2023selfinstruct}.
% To note a few, \citet{srivastava2023beyond} famously benchmarked ICL for many tasks and models.
% \citet{perez2021true,Lu2022FantasticallyOP} showed the sensitivity of ICL to the choice of demonstrations and their orderings.
% \citet{shin2022effect,razeghi2022impact} showed the sensitivity of ICL performance to the frequency and size of the relevant pretraining corpus.
% % \citet{wang2023selfinstruct} show the feasiblity of LLM alignment with its own feedback, enabled by the guidance provided by ICL.
% \citet{shen2023flatnessaware} treat the ICL prompt selection as an optimization problem. \citet{pan2023incontext} disentangle task recognition and task learning in ICL, which is analyzed in theory recently by \citet{lin2024dual}. These works highlight numerous ways the ability of models to perform ICL changes under different conditions but do not attempt to explain how it functions.

\paragraph{Ciphers and their use in AI:} The problem of deciphering substitution ciphers is studied in NLP as it may provide automatic ways to  decipher lost languages without any  parallel corpus~\cite[\emph{inter alia}]{knight2006unsupervised,ravi2008attacking,ravi2011bayesian,dou2012large,berg2013unsupervised,pourdamghani2017deciphering,nuhn2013beam,berg2011simple,corlett2010exact,aldarrab2020can}.
% \citet{knight2006unsupervised} presented an unsupervised approach based on Expectation-Maximization~\cite{dempster1977maximum} to solve language decipherment tasks.
For instance,
\citet{ravi2011bayesian} introduces a  Bayesian approach for deciphering  substitution ciphers, combining information from letter n-gram language models and word dictionaries to perform efficient sampling-based inference for decipherment results.
We also note various optimization-based and heuristic-based computational frameworks that are deterministic in nature for deciphering substitution ciphers~\cite{peleg1979breaking,ganesan1993statistical,olson2007robust}.

We also note the work of
\citet{yuan2023gpt} which is the only work (that we know of) applying ciphers on LLMs (GPT-4, specifically) in the context of safety problems, which is a different focus than ours.

% showed that GPT-4~citep{} is able to resolve/recover non-natural languages-ciphers.


% Historically popular for its simplicity, hwow
% In the modern era, simple substitution ciphers hold limited practical use for secure communication due to their vulnerability to cryptographic attacks.

% including by Julius Caesar (the Caesar cipher), but it is vulnerable to frequency analysis, making it easily crackable with modern techniques.

% Historically popular for its simplicity, as used by Julius Caesar, this cipher is susceptible to frequency analysis and easily broken with modern methods. Today, simple substitution ciphers are largely impractical for secure communication due to their vulnerability to cryptographic attacks, but they are valuable for teaching basic cryptography concepts and are occasionally used in puzzles or recreational cryptography.


\paragraph{Alternative explanations of ICL:} Since the discovery of ICL~\citep{brown2020language}, numerous studies have explored it across various contexts~\citep{zhao2021calibrate,min2022rethinking,mishra2022reframing,han2023understanding,wang2023selfinstruct,sia2024does,vacareanu2024words,mueller2024context}. For example, \citet{perez2021true,Lu2022FantasticallyOP,mishra2022reframing} demonstrated ICL's sensitivity to the selection and sequence of demonstrations, while \citet{shin2022effect,razeghi2022impact} highlighted its sensitivity to the frequency and size of the relevant pre-training corpus. Another research direction seeks to elucidate the mechanisms behind ICL. \citet{xie2021explanation} described ICL as implicit Bayesian inference, where ICL demonstrations are mapped to a latent concept (task) learned during pre-training. Other works have attempted to explain ICL as a form of implicit optimization (gradient descent and its variants)~\citep{garg2022can,zhang2023trained,dai2022can,akyurek2022learning,von2023transformers,li2023closeness}, though the applicability of these formalisms to real LLMs is debated~\citep{shen2024icl_vs_gd}. A few studies aim to understand how ICL emerges in LLMs. \citet{hahn2023theory} suggested that the compositional structure of natural language leads to emergent in-context learning, while other works~\citep{chan2022data} propose that certain distributional properties in the pre-training data may give rise to ICL. Although many of these studies explain certain aspects of ICL, they fall short in others. The precise origins of ICL in LLMs remain an active area of research.


% \paragraph{Other explanations of ICL:} Ever since In-Context Learning was discovered~\citep{brown2020language}, multiple works have studied it under diverse settings~\citep{zhao2021calibrate,min2022rethinking,mishra2022reframing,han2023understanding,wang2023selfinstruct,sia2024does,vacareanu2024words,mueller2024context}.
% For instance, \citet{perez2021true,Lu2022FantasticallyOP,mishra2022reframing} showed the sensitivity of ICL to the choice of demonstrations and their orderings; \citet{shin2022effect,razeghi2022impact} showed the sensitivity of ICL performance to the frequency and size of the relevant pre-training corpus.
% Another line of study aims to explain how ICL actually works.
% For instances,  \citet{xie2021explanation} explained ICL as implicit Bayesian inference, mapping a ICL demonstrations to a latent concept (task) learned via pre-training.
% Others have tried to explain ICL as an implicit optimization (gradient descent and variants)~\citep{garg2022can,zhang2023trained,dai2022can,akyurek2022learning,von2023transformers,li2023closeness} however, the validity of these formalisms in LLMs is debated~\citet{shen2024icl_vs_gd}.
% Few works try to understand \emph{how ICL emerges in LLMs}. \citet{hahn2023theory} posited that compositional structure in natural language gives rise to emergent in-context learning. Other works~\citep{chan2022data} theorize more distributional properties in the pre-training data, that may give rise to ICL. Many of these works explain some properties of ICL, but fail at others.
% The exact origin of ICL in LLMs still remains an active area of study.






\section{Problem Setup}
\label{sec:setup}


\subsection{Preliminaries: In-Context Learning}
% In this section, we describe a framework to view In-Context Learning as a stochastic process that induces an information flow between probability masses of tokens on the basis of inputs seen.
% \subsection{ICL Cipher}
% \paragraph{In-Context Learning:}
Let $f_\theta$ denote a pre-trained language model parameterized by $\theta$. This model performs ICL by conditioning on an ordered set of $n$-many input-output pairs $D_\text{demo}= (x_1, y_1, x_2, y_2, \hdots, x_n, y_n)$.
To measure this model's competence, we evaluate it on a collection of input-output pairs $D_\text{test} = \{(x_i, y_i)\}$. Specifically, for instance $(x_\text{test}, y_\text{test}) \sim D_\text{test}$, from an LM conditioned on the demonstrations with an appropriate encoding:
$y_\text{pred}\sim f_\theta(D_\text{demo}, x_\text{test})$ we extract a predicted label $y_\text{pred}$ which is then compared against the gold label  $y_\text{test}$.


\subsection{ICL via \textsc{Bijective} Ciphers}
\label{subsec:iclciphers}
We formally define a bijective encoding of prompts.
\paragraph{Substitution cipher:}
\label{sec:sub-cipher}
A simple substitution cipher is a technique for encoding messages in which each letter in the plaintext is substituted with a different letter from the alphabet, usually according to a predetermined rule or key. Formally, we can define a cipher function \( c: V \rightarrow V \) that maps each [sub]word in the lexicon \( V = \{t_j\}_{j=1}^{|V|} \) to another [sub]word. In the special case where this cipher function is the identity function (i.e., \( c(t_j) = t_j \) for all \( j \)), no changes occur in the text.
% \aayush{i have described the cipher as a probability distribution following this (will help in computing bounds). @DK, do you think this both these can be aligned?}
However, if for certain letters or [sub]words the function \( c \) maps them to different counterparts, the plaintext is transformed into a ciphertext that conceals the original message. We define \textit{shuffle rate} $r \in [0, 1]$ as the proportion of [sub]words in the lexicon that are mapped to a different [sub]word. If the lexicon has $100$ [sub]words, a shuffle rate of $0.1$ will randomly select $10\%$ of these [sub]words \(S \subseteq V\) and create a substitution mapping ($c$) for this subset.
The set $S$ is the set of ciphered tokens. We define this set only for bookkeeping, to keep track of what tokens are ciphered.
Next, we describe how this key is generated.

%The transformation relies on this unique key used, which must be known to decode the message back into its original form.

\paragraph{\textsc{Bijective} cipher:}
We introduce the substitution cipher in the inputs $(x_1, \hdots, x_n)$ of ICL demos.
Given a set $S$ of tokens to be shuffled, we create a bijective mapping between two permuted orders of $S$.
For example, say the token ``school'' is mapped to ``apple'', as illustrated in~\autoref{fig:pipeline}.
Let the input $x_i$ be constituted of $K_i$ tokens, i.e., $x_i$ is the ordered sequence of tokens $(t_1, \hdots, t_{K_i})$.
For all $t_j = \text{school} \in x_i$ or $x_\text{test}, c(t_j) = \text{apple}$. This results in corresponding ciphered inputs $x_i'$ or $x_\text{test}'$. Moreover, as $c$ is a bijection, $\exists\hspace{2pt}c^{-1}$ such that for all $t_j = \text{apple} \in x_i'$ or $x_\text{test}', c^{-1}(t_j) = \text{school}$.

\paragraph{Decipherability of \textsc{Bijective} cipher:} Let the actual function between all $(x_i, y_i)$ pairs be $h$, i.e. $h(x_i) = y_i, \forall (x_i, y_i) \in D_\text{demo} \cup D_\text{test}$. Using ICL, the model $f_\theta$ employs both TR and TL to approximate $h' \approx h$ such that $h'(x_i) \approx y_i$. This original function $h$ can not be expected to work on ciphered (or shuffled) inputs $x_i'$. However, there is a corresponding function $g = h(c^{-1}(x_i'))$ that is equivalent to $h(x_i)$.
% , as $r$ increases from $0$ to $1$, more and more memorized correlations from pre-training are lost, i.e. TR capabilities are lost
This shows that $h$ is still recoverable from the ciphered inputs. In natural language, replacing a word with another fixed but randomly decided word can completely change the meaning of its context. Any TR capabilities are expected to be severely hurt with ciphered inputs. To perform well on $D_\text{test}$, the model has to rely heavily on TL to learn and perform this composite function.

\subsection{ICL via \textsc{Non-Bijective} Ciphers}
\label{subsec:uninformative}

% \paragraph{\textsc{Non-Bijective} cipher:}
For comparison with Bijective ciphers~(\S\ref{subsec:iclciphers}) we also create a random substitution cipher which replaces all tokens ($\in S$) from the inputs $x_i$ or $x_\text{test}$ with tokens picked uniformly at random from $S$, i.e., $c(x_i) \sim \texttt{uniform}(S)$.

\paragraph{Indecipherability of \textsc{Non-bijective} cipher:} As $c$ is not surjective anymore, $c^{-1}$ does not exist. This implies that a composite function through which $h$ can be recovered also does not exist. In this case, TL can no longer work.

\subsection{Measuring ``Learning'' via \name{}}
\label{subsec:learning:measuring}

% \paragraph{Measuring ``learning'':}
Our two ciphers (\S\ref{subsec:iclciphers} and \S\ref{subsec:uninformative}) can be used to study the TL capabilities of LLMs. As the \textsc{Non-bijective} cipher removes learnable patterns in the ICL Demos, the performance of LLMs on \textsc{Non-bijective} ciphered text can act as a baseline to measure any remaining impact of TR. The gap between the performance of LLMs on \textsc{Non-bijective} and \textsc{Bijective} ciphered text can be a practical measure of TL.





% \paragraph{Substitution strategy:}
% \label{subsec:shuffling}
% After shuffled the original vocabulary following the above principles and constraints, we obtain the shuffled vocabulary.
% All the tokens in the shuffled vocabulary have a one-to-one
% correspondence with the tokens in the original vocabulary, namely a bijective mapping.
% We include two substitution strategies for encrypting ICL prompt\fzx{better expression?} in our experiments: bijective substitution and random substitution.
% % For original substitution, we don't do any changes to the original prompt, namely substituting the original tokens with themselves.
% For bijective substitution, we substitute the original tokens with shuffled tokens according to the bijective mapping.
% For random substitution, we substitute the original tokens that don't correspond to themselves with a random token. \fzx{similar frequency?}
% \aayush{we don't need this paragraph now, as it is explained in section 3. rest of 4.4 fits here better than 3.}



% \aayush{this maybe tricky to justify. The cases $m>n, m<n$ can probably be pushed to appendix.}

% \daniel{I think 4.4 and 4.5 should move to Sec 3}\aayush{I think they align more with section 4. 4 is about implementation details. 3 is motivation for that}

% % Our ICL Cipher
% \begin{itemize}
%     \item \textsc{Bijection}: We create a unique bijection between the elements of $S$. All inputs of ICL demos are then shuffled according to this bijection. For example, say the token ``school'' is mapped to ``apple'' (both $\in S$). Each $t_j = \text{school} \in x_i$ or $x_\text{test}$, will be replaced with ``apple'' and vice-versa. All other tokens will remain unchanged. Therefore, the cipher can be defined as a probability distribution over the cross set $V \times V$ denoting the probability of shuffle between each pair, such that $P(t_1, t_2) = P(t_2, t_1) = 1$ if $t_1, t_2 \in S$ and $t_1$ is mapped to $t_2$. In all other cases, $P(t_1, t_2) = 0$.
%     \item \textsc{Non-bijective}: The elements of $S$ are randomly shuffled with a different token from $S$. So, if ``school'' $\in S$, it will be replaced with another token from $S$, uniformly picked at each occurrence. The cipher is defined as $P(t_1, t_2) = \frac{1}{|S|} \forall t_1, t_2 \in S$. In all other cases, $P(t_1, t_2) = 0$.
% \end{itemize}

% \paragraph{In-context Learning under substitution cipher:}
% We induce the substitution cipher in the inputs $(x_1, \hdots, x_n)$ of ICL demos. Let the input $x_i$ be constituted of $K_i$ tokens (sub-words), i.e., $x_i$ is the ordered sequence of tokens $(t_1, \hdots, t_{K_i})$. We create two types of ciphers.
% \begin{itemize}
%     \item \textsc{Bijection}: We create a unique bijection between the elements of $S$. All inputs of ICL demos are then shuffled according to this bijection. For example, say the token ``school'' is mapped to ``apple'' (both $\in S$). Each $t_j = \text{school} \in x_i$ or $x_\text{test}$, will be replaced with ``apple'' and vice-versa. All other tokens will remain unchanged. Therefore, the cipher can be defined as a probability distribution over the cross set $V \times V$ denoting the probability of shuffle between each pair, such that $P(t_1, t_2) = P(t_2, t_1) = 1$ if $t_1, t_2 \in S$ and $t_1$ is mapped to $t_2$. In all other cases, $P(t_1, t_2) = 0$.
%     \item \textsc{Non-bijective}: The elements of $S$ are randomly shuffled with a different token from $S$. So, if ``school'' $\in S$, it will be replaced with another token from $S$, uniformly picked at each occurrence. The cipher is defined as $P(t_1, t_2) = \frac{1}{|S|} \forall t_1, t_2 \in S$. In all other cases, $P(t_1, t_2) = 0$.
% \end{itemize}

% \paragraph{Decipherability} Now, we discuss what substitutions are decipherable and how they would impact ICL. Let the actual function between all $(x_i, y_i)$ pairs be $h$, i.e. $h(x_i) = y_i \forall (x_i, y_i) \in D_\text{demo} \cup D_\text{test}$. Using ICL, the model $f_\theta$ employs both TR and TL to approximate $h' \approx h$ such that $h'(x_i) \approx y_i$. During shuffling of inputs $x_i$, as $r$ increases from $0$ to $1$, more and more memorized correlations from pre-training are lost, i.e. TR capabilities are lost. This is because tokens important for $h$, according to the pre-trained model, may be replaced with completely unrelated tokens (even with bijection cipher as the bijection is randomly chosen for $S$). However, the random and bijection cipher present a difference for TL capabilities. To analyze TL individually, we make a simplifying assumption.

% Let $F(x_i) \in \mathbb{R}^{|V|}$ denote the frequency distribution of tokens in $x_i$.

% \begin{assumption}
% \label{ass:freq}
% $h$ is a function of $F(x_i)$.
% \end{assumption}

% Assumption~\ref{ass:freq} limits the complexity of functions $h$ studied under ICL settings. It implies that a simple look at $F$ can uniquely determine $h$. Although, this setting is We use this family of functions to find an upper bound on the the gap between bijection and cipher decipherability.

% \begin{theorem}
%     $\forall h$ $\exists g$
% \end{theorem}

% With the bijection cipher, $\forall h \exists g$ such that, the frequency distributions uniquely determining $h$ have corresponding distributions where the frequency of original and shuffled tokens from $S$ are exchanged, resulting in a unique determination of $g$. This holds for all values of $r \in [0, 1]$. \aayush{write as theorem?}


% \begin{figure*}[t]
% \includegraphics[width=\textwidth]{fig/ciphers.png}
% \caption{
% Token frequency distributions change differently for bijection and random cipher. The example above shows the case when $|S| = 2 \ll |V|$. When $r=1$ ($S = V$), random cipher will produce a uniform distribution for every input.
% }
% \label{fig:ciphers}
% \end{figure*}

% However, with the random cipher, information necessary for determination of $h$ from original inputs, is diffused uniformly among all tokens in $S$. This results in part of the frequency distribution uniquely determining $h$ being lost. Now, the existence of a corresponding uniquely determined $g$ is unclear. See Figure~\autoref{fig:ciphers} for a visual explanation. In this case, as $r$ increases from $0$ to $1$, all functions $h$ end up being replaced by a single function that assigned equal probabilities to all $y_i$ for all $x_i$. \aayush{again, do we write as theorem?}

% This implies a theoretical gap \aayush{I think writing down a theoretical bound will be beneficial here, will do soon.} between the performance of $h'$, the approximation to $h$ that the model learns (using TL), under the random and bijection ciphers. As $r$ increases from $0$ to $1$, the ICL performance of $h'$ should remain constant under the bijection cipher, but be reduced to baseline random guessing under the random cipher. \aayush{I am abstracting out the learning mechanism here (to be perfect)}. In practice, multiple other factors determine the gap. The functions $h$ are not as simple as in assumption~\ref{ass:freq} where shuffles significantly hurt the identifiability of $h$. Therefore even bijection ciphers may lose a lot of ICL performance. Moreover, we assumed that ICL (using TL) can perfectly decipher the shuffle that we introduced. This is unclear as the exact learning mechanism of ICL is unknown. Therefore, we expect to see a lot more retention in performance of ICL under the bijection cipher compared to the random cipher, but the exact gap may not be predictable under this setup.

%%%%%%%% TEXT BELOW IS OLD

% \paragraph{In-Context Learning:} Let $f_\theta$ denote a pre-trained language model parametrirzed by $\theta$.
% This model performs ICL by conditioning on an ordered set of $n$-many input-output pairs $D_\text{demo}= (x_1, y_1, x_2, y_2, \hdots, x_n, y_n)$.
% To measure this model's competence, we evaluate it on a collection of input-output pairs $D_\text{test} = \{(x_i, y_i)\}$. Specifically, for instance $(x_\text{test}, y_\text{test}) \sim D_\text{test}$, from an LM conditioned on the demonstrations with an appropriate encoding:
% $y_\text{pred}\sim f_\theta(D_\text{demo}, x_\text{test})$ we extract a predicted label $y_\text{pred}$ which is then compared against the gold label  $y_\text{gold}$.

% \paragraph{Substitution cipher:}
% A simple substitution cipher is a technique for encoding messages in which each letter in the plaintext is substituted with a different letter from the alphabet, usually according to a predetermined rule or key. Formally, we can define a cipher function \( c: V \rightarrow V \) that maps each [sub]word in the lexicon \( V \) to another [sub]word. In the special case where this cipher function is the identity function (i.e., \( c(v_i) = v_i \) for all \( i \)), no changes occur in the text. However, if for certain letters or [sub]words the function \( c \) maps them to different counterparts, the plaintext is transformed into a ciphertext that conceals the original message. This transformation relies on the specific rule or key used, which must be known to decode the message back into its original form.

% Here define shuffle rate to be the proportion of letters or [sub]words in the lexicon \( V \) that are mapped to different counterparts by the cipher function \( c \). A shuffle rate of 0 indicates that the cipher function is the identity function, resulting in no transformation of the plaintext, while a shuffle rate of 1 indicates that every element in the lexicon is substituted, resulting in a complete transformation of the plaintext into ciphertext.
% \aayush{I thought the shuffle rate was: how many tokens from the original text (not the lexicon) would be substituted. We always create a mapping for the whole lexicon, but only shuffle a percentage of the tokens in the presented text.}\daniel{You're right. Will fix.}\aayush{Actually I talked with Zhouxiang, and he says that he is creating a mapping for a percentage of the tokens. So you're right. This approach is easily reproducible with zipfian which we settled on.}

% A simple substitution cipher is a method of encoding where each in the plaintext is replaced with another letter from the alphabet, typically following a fixed rule or key.
% Formally, define a cipher function $c: V \rightarrow V$ that maps each [sub]word in the lexicon $V$ to another [sub]word.
% In the special case where this cipher function is identify (i.e, $c(v_i) = v_i, \forall i$) it does not lead to any changes.
% However, if for certain


% \paragraph{Reversing the cipher:}
% A cipher is trivially reversible if it is bijective (i.e., each element of the plaintext maps to a unique ciphertext element, and each ciphertext element maps back to a unique plaintext element). This ensures that there is a one-to-one correspondence between the plaintext and ciphertext, allowing the original message to be easily recovered.





% \paragraph{In-context Learning under substitution cipher}
% Given a set of demonstrations $D_\text{demo}= (x_1, y_1, x_2, y_2, \hdots, x_n, y_n)$, we induce the substitution cipher in the inputs $(x_1, ..., x_n)$. Let the input $x_i$ be constituted of $K_i$ tokens, i.e., $x_i$ is the ordered sequence of tokens $(t_1, t_2, ..., t_{K_i})$. Now,
% \begin{itemize}
%     \item \textit{Bijection Shuffle}: A subset of the vocabulary $r \in [0, 1]$ (shuffle rate) is mapped uniquely to other tokens, creating a bijection. For example, say the token ``school'' is mapped to ``apple''. Each $t_j = \text{school} \in x_i$ will be replaced with ``apple'' and vice-versa. All other tokens will remain unchanged.
%     \item \textit{Random Shuffle}: The subset is mapped randomly to other tokens. If ``school'' is shuffled, it will be replaced with another random token at each occurrence, making it impossible to decipher any reversal (given enough shuffles).
% \end{itemize}





% \begin{assumption}
% perfect model, knows true distribution of all texts.
% \end{assumption}
% \aayush{all texts or pretraining text?, i think the latter because then it would also know the distribution of the shuffled text.}

% \begin{definition}
% definition of learning: information flow between random variables (original token's probability mass $\rightarrow$ shuffled token's probability mass, under substitution cipher based interventions.  $p(\text{original\_token})\rightarrow p(\text{shuffled\_token})$
% \end{definition}
% \aayush{needs revision}

% \begin{assumption}
% the flow is a function of number of observations, exponential decay. more times you see, more confident the model gets of the shuffle (diminishing returns).
% \end{assumption}
% \aayush{to think: are some shuffles more easily understood than others, does it depend on frequency, etc?}

% \begin{assumption}
% the remaining tokens remain unchanged in all contexts. $p(t_{\text{others}} | t_1, ..., t_n)$ is fixed.
% \end{assumption}

% \begin{theorem}
% can not transfer more than $\delta$ information for $n$ demonstrations. (derive probability mass under the exponential decay assumption)
% \end{theorem}

% \aayush{@DK, can you see if this plan looks reasonable?}


\section{Experimental Setup}
\label{sec:setup}
We evaluate a range of pretrained LLMs on four datasets to study \name.
% We present an overview (\S\ref{subsec:overview}), describe the models (\S\ref{subsec:models}), datasets (\S\ref{subsec:datasets}) and prompt template (\S\ref{subsec:prompt}) used in our experiments.
% As shown in \autoref{fig:pipeline}, and described in \S\ref{sec:sub-cipher},
We apply \textsc{Bijective} and \textsc{Non-bijective} Ciphers on the vocabulary of multiple LLMs for a variety of shuffle rates.
% we first apply bijective shuffling with certain shuffle rate to the original token-level vocabulary of the LLM, which is defined by the model's tokenizer. (See detailed process in \ref{sec:sub-cipher})
% More formally, if the shuffle rate is $r$ and the vocab size if $V$, we randomly pick $rV$ tokens then do bijiective shuffling within them.
% Through bijective shuffling, we ensure that all the tokens in the shuffled vocabulary have a one-to-one correspondence with the tokens in the original vocabulary.
% (The unpicked tokens correspond to themselves in the shuffled vocabulary)
% We then use the ciphered ICL prompts without any additional instructions to evaluate the performance, and in turn quantify the TL capabilities of these LLMs on various tasks.
We then use the difference between these two to quantify a proxy for TL capabilities of these LLMs on various tasks.




\subsection{Design Choices for \name}
% \subsection{Experiment Design}
% \subsection{Empirical Considerations for Implementing \name}
\label{subsec:empirical}
% Here, we describe some nuances for shuffling vocabulary and sampling ICL examples.



\paragraph{Zipfian shuffling:}
Literature has shown a strong correlation between token frequency in the pre-training corpus and model performance \citep{razeghi2022impact,mallen2022trustlmretrieval}---LLMs tend to perform better on frequent tokens.
To diminish the confounding influence of token frequency, we constrain the shuffling between tokens of similar frequency.
Inspired by Zipfian shuffling \cite{piantadosi2014zipf},
% \fzx{could someone check this cite? i am not sure if zipf = zipfian}\aayush{checked. looks ok to me.}
 we divide all the tokens into $k$ ($k=10$ in our experiments) groups of similar frequency and shuffle the tokens within each chunk. Since the pre-training corpora are usually not accessible for LLMs, we use a representative external corpus to approximate the real token frequency. Specifically, we use the Wikipedia~\cite{wikidump} to calculate token frequency instead, which is an approximation to the actual token frequency.

\input{table/fixed_shot_r}


% \aayush{I am renaming this to priority sampling to avoid confusion with uninformative cipher}
\paragraph{Priority sampling of ICL demos:}
\label{subsec:sampling}
% \daniel{Better phrasing: informative sampling}
To create an ICL demo set, one way to do it is randomly sample the required number of examples (say $n$) from the pool of demos. We call this \textbf{non-priority} (random) sampling.
However, in practice we always perform \textbf{priority sampling} (unless otherwise specified) where we prioritize examples that contain
% demonstrate
the substituted tokens of the test case input.
This is done to expose LLMs to the relevant substitutions from which they can learn to decipher.
Suppose the number of tokens to be shuffled in the test input is $m$ (which depends on the shuffle rate $r$).
% For a given shuffle rate $r$, say the number of tokens to be shuffled in the test input $x_t = \{t_1,t_2\hdots t_{K_t}\}$, are $m$.
% of the $K_i$ tokens $\in S$ and are to be substituted,
The goal is to select $n$ demonstrations from the pool of demos, such that each of them contain at least one of the $m$ uniquely ciphered (substituted) tokens.
This is trivial if $m=n$ (i.e., $n$ demos cover the whole set of $m$ substitutions). Otherwise:
% \daniel{In previous sentence, we use $m$ to denote two diffeent notions? If so, fix. }
% See more details in Appendix \ref{appendix:informative-sample}. \aayush{we may want to pull content from the appendix here, as suggested by one of the reviewers. we have enough space.}
% As a limited number of demos cannot cover the whole set of substituions in the vocabulary, this is done to reliably expose LLMs to the relevant substitutions, from which they can learn to decipher.
% Depending on the dataset, our demonstration pools have different sizes.
% , which are much bigger than n.
% The goal is to select $n$ demonstrations in a way that prioritizes the  $m$  ciphered (replaced) tokens in the test input.
\begin{itemize}[noitemsep,leftmargin=10pt,topsep=1pt]
 \item   If $m < n$ (i.e., the number of substitutions are less than the required number of ICL demos to be sampled from the pool), we choose $m$ examples according to priority sampling and the rest of $n-m$ examples are randomly picked from the demo pool.
 \item  If $m > n$, we select a random subset of the ciphered tokens of size $n$. For each of these cases, we randomly sample a demonstration.
 % we apply priority sampling for only $n$ out of $m$ cases chosen randomly (we can not guarantee the illustration of all substitutions in this case).
\end{itemize}
We always use priority sampling (unless otherwise specified). However, in \S\ref{appendix:infor-random} we compare priority sampling with non-priority (random) sampling.

% \daniel{in addition or throughout the experiments?}\aayush{I think we have results for both. Not sure if we are showing the random sampling results in main text.}
% \daniel{@Zhouxiang, once you add the other results to the appendix, revise this phrasing so that it's precise. You may say at this end of this parargaph: "For all the scenaiors in the main text we use informative sampling. However, in \S\ref{} we compare these two sampling strategies and show that they perform comparably."}

\input{table/llama8b_infor_sample}

\begin{figure*}[h]
\centering
\includegraphics[width=0.48\textwidth]{fig/SST-2 llama3 Bijective Sub and Informative Sampling.pdf}
\hfill
\includegraphics[width=0.48\textwidth,trim=0cm 0cm 0cm 0cm]{fig/SST-2 llama3 Gap with Informative Sampling.pdf}
\caption{
\textbf{Left:} Accuracy of LLaMa 3.1 8B on SST-2 dataset.
% with informative sampling \aayush{should we remove "informative sampling" from here, to avoid confusion?}.
With the \textsc{Bijective} cipher, accuracy decreases smoothly as the shuffling rate increases, highlighting the difficulty in interpreting the ciphered text. However, with more demonstrations, accuracy increases, suggesting that the model's ability to solve \name{} improves with additional examples. \textbf{Right:} The $y$-axis displays the accuracy gap between \textsc{Bijective} and \textsc{Non-bijective} ciphers. This small but consistent gap illustrates the model's ability to decipher \textsc{Bijective} maps. For very high shuffle rates (e.g, $> 0.8$) tasks become very hard to understand (for the model and even humans). So the performance numbers at these shuffle rates become noisy.
% as the task is mostly noise (and basically no signal). So better to ignore that range.
% , indicating that \textbf{LLMs can decipher randomly generated \textsc{Bijective} ciphers but not }
\aayush{@zhouxiang: for the figure on the right, can we mark the 0 line with \textsc{Non-bijective} and the solid lines with \textsc{Bijecitve}, and make the y-axis label like "difference in performance" and positive numbers like "+0.02", "+0.04", etc. It will make the figure more readable.}.
% \textbf{Left:} The accuracy of \textsc{Bijective} cipher  changes smoothly while varying the shuffling rate. We also see that with demonstrations, the accuracy moves higher, indicating that model's ability to better solve \name{} with more demonstrations.
% \textbf{Right:} The $y$-axis shows the accuracy gap between \textsc{Bijective} and \textsc{Non-bijective} ciphers. This gap, while small, is consistently positive indicating that LLMs are able to solve \name{} better than the random baseline.
% \daniel{For very high shuffle rates (e..g, $> 0.8$) the task becomes very hard to understand (for the model and even humans) as the task is mostly noise (and basically no signal). So better to ignore that range.}
}\label{fig:sst-curve}
\end{figure*}

\paragraph{Shuffle Rate:}
The shuffle rate parameter \(r\) determines the proportion of tokens that are replaced.
When \(r\) is close to 0, the cipher's effect is minimal, as few or no tokens are substituted, making it uninteresting.
Conversely, when \(r\) approaches 1, nearly all tokens are shuffled, solving the task nearly impossible (under both \textsc{Bijective} and \textsc{Non-bijective} ciphers).
Thus, our focus lies on a moderate shuffle rate between 0 and 1, striking a balance between these extremes.
We analyze its effect in \S\ref{subse:shuffle-rate}.


\paragraph{Special tokens and filters:}
LLMs usually have a list of special tokens that help the model understand the prompt and task (e.g. next token prediction).
For example, Llama3.1 models use  \texttt{<|begin\_of\_text|>} and \texttt{<|end\_of\_text|>} to denote the start of input and end of generation.
We preserve special and punctuation tokens from getting ciphered to avoid hurting models' basic functionality. (Full list of preserved tokens are in Appendix~\ref{appendix:reserved}). Similarly, we avoid disturbing spaces in the original text (details in Appendix~\ref{appendix:space}).

% \paragraph{Handling of white spaces:}

\subsection{Models}
\label{subsec:models}
We focus on pretrained LLMs in our experiments, including Llama 3.1 \cite[\texttt{Llama-3.1-8B}]{dubey2024llama3herdmodels}, \qwen \cite[\texttt{Qwen2.5-7B}]{qwen2.5}, \olmo \cite[\texttt{OLMo-7B-0724-hf}]{Groeneveld2023OLMo} and \gemma \cite[\texttt{Gemma-2-9b}]{gemma_2024}.
We don't do experiments on aligned (instruction-tuned or RLHF-ed) models as prior work shows that alignment trades typically hurts in-context learning performance~\cite{fu2022does}.
% \fzx{multilingual}
% \fzx{checkpoints}

% upon alignment thesne models lose their emergent ability to follow demonstrations.
% instruction-tuning and RLHF could hurt ICL\fzx{cite}.




\subsection{Datasets}

\label{subsec:datasets}
We conduct experiments on four datasets.
SST-2 \cite{socher2013recursive} and Amazon \citep[Amazon Reviews 2023]{hou2024bridging} are for binary sentiment classification task.
HellaSwag \cite{zellers2019hellaswag} is for sentence completion task, formatted as four-choices QAs.
WinoGrande \cite{sakaguchi2020winogrande} is for pronoun resolution task, formatted as binary-choice QA.
For each dataset, we curate a demo pool for sampling ICL demos and a test set contain to-be-tested cases.
We use accuracy as the metric for all our experiments if not specified.
We averaged the metrics across three runs of experiments for a more reliable evaluation.
Further details on datasets (prompts and examples) are in Appendix.
% Appendix \ref{appendix:examples} shows example inputs/outputs for all the datasets.



% \daniel{Somewhere we should say that our experiments are average of multiple runs to obtain low variance trends (if we don't say this already)}\aayush{@ZF, I also wanted to confirm that we were indeed averaging each experiment across multiple runs (with different seeds). May be one reason why the plots are sharp.}\fzx{yes, average of 3 runs with random seeds}

% \section{Empirical Findings}
\section{Evidence of Task-Learning in ICL}
% \subsection{LLM's Ability to Decipher \textsc{Bijective} Ciphers}
\label{results:llm:solve:it}
\label{results}
% \subsection{LLM's Ability to Decipher \textsc{Bijective} Ciphers}
% \subsection{Model stay above random}



\autoref{tab:fixed_shot_r} shows the performance of fours LLMs on four datasets ciphered using \name{}.

\paragraph{LLMs can decipher \textsc{Bijective} Ciphers:}
We see a consistent improvement in the performance of LLMs on \textsc{Bijective} ciphered inputs over \textsc{Non-bijective} ciphered inputs (except for \olmo on SST-2). With fixed shuffle rate and number of demonstrations, any influence of \textbf{task retrieval} on the model performance remains the same for both ciphered inputs. However, the consistent gap clearly demonstrates that the model understands decipherable \textsc{Bijective} maps better than the undecipherable \textsc{Non-bijective} maps. This provides evidence for exclusively \textbf{task learning} capabilities of LLMs.


% \paragraph{LLMs can decipher \textsc{Bijective} Ciphers:}
% % \paragraph{LLMs show consistent (but small) ability to   decipher Bijective Ciphers:}
% % \paragraph{Evidence of task-learning in ICL:}
% From \autoref{tab:fixed_shot_r}, we observe a consistent gaps between bijective ciphers and random ciphers, except for \olmo on SST-2.
% This (to some extent) indicates that decipherability is universal, instead of being limited to a certain model on a certain dataset.




% \section{Further Analysis}
\section{Further Empirical Analysis}




\subsection{Effect of Shuffle Rates}
\label{subse:shuffle-rate}
\autoref{fig:sst-curve} illustrates the performance of \llama~on SST-2 dataset with priority sampling.
We can observe a consistent and clear gap between \textsc{Bijective} ciphers and \textsc{Non-bijective} ciphers across a range of shuffle rates, indicating the model's ability to decipher bijections.
% \daniel{Our experiment results show that even when the shuffle rate reaches 1.0, LLMs still consistently demonstrate above-random performance under bijective ciphers, which provides better and stronger evidence for task “learning”. }


\subsection{Effect of Number of Demonstrations}
In \autoref{tab:llama8b_infor_sample}, we present the gap in performance between \textsc{Bijective} and \textsc{Non-bijective} ciphers under the effect of number of ICL demos.
% on  has an increasing trend with more demonstrations.
Overall, the \textsc{Bijective} cipher consistently outperforms the \textsc{Non-bijective} cipher across different numbers of demonstrations.
% however, their trends is not very clear.
Increasing the number of demonstrations generally results in a larger gap between \textsc{Bijective} and \textsc{Non-bijective} ciphers. However, beyond a certain threshold, this effect plateaus, and additional demonstrations have a diminishing impact.
% The effect is more pronounced with small number of demos, and the increment flattens with high number of demos.
% This suggests that with more demonstrations, we see more learning through TL.
\autoref{fig:sst-curve} (on the right) also shows this visually for SST-2 dataset.


\subsection{Effect of Models Size}
% We have also explored the effect of Model size.
\autoref{fig:model_size} shows the effect of model size on this gap.
As the model size increases, performance for both \textsc{Bijective} and \textc{Non-bijective} ciphers improve, but the gap between them remains existent.
% From~\autoref{fig:sst-curve}, it is clear that although \textsc{ICL Ciphers} hurt the performance of LLMs, they still perform better than random for even high shuffle rates. Moreover, the performance of LLMs on \textsc{Bijection} ciphered inputs is consistently higher than those of \textsc{Non-bijective} ciphered inputs. This implies that the models can decipher (to some extent) the reversible ciphers, exhibiting their ability to solve novel tasks.
This indicates that decipherability of \textsc{Bijective} ciphers exists across models of different sizes.
% , instead of being limited to a certain model.


\subsection{Restricting the Space of Cipher}
\label{sec:noun:experiments}
% \paragraph{Constrain Shuffling to Nouns}
We notice that the gaps on HellaSwag and WinoGrande are smaller than those in SST-2 and Amazon. The reason behind it could be the complexity of these two datasets, which could impact the model's ability to solve the ciphers.
% , thus making the gaps less significant.
To verify this, we constrain the vocabulary shuffling to only nouns on these two datasets.
\autoref{tab:llama8b_noun} shows that the gap between \textsc{Bijective} and \textsc{Non-Bijective} ciphers moderately increases for noun-constrained shuffling. This means that the model is more effectively learning to solve ICL ciphers.
\fzx{or should we say it's around the same}

\input{table/shuffle_noun}

\begin{figure*}[h]
\centering
\includegraphics[width=0.48\textwidth]{sst-2_8b_70b_informative_performance_comparison.pdf}
\includegraphics[width=0.48\textwidth]{amazon_8b_70b_informative_performance_comparison.pdf}
\caption{Accuracy comparison of Llama-3.1-8B and Llama-3.1-70B models on SST-2 (left) and Amazon (right) datasets under \textsc{Bijective} and \textsc{Non-bijective} ciphers. The experimental setting is 20-shot with $r=0.6$. Larger models outperform smaller ones under both ciphers, but
\textsc{Bijective} setting consistently yields higher accuracy.
 % \fzx{change description}
}
\label{fig:model_size}
\end{figure*}

% \paragraph{Ability to solve \name{} improves with model size:} As seen in~\autoref{fig:model_size}, larger model outperforms smaller model when solving \name{}. For SST2 dataset, the performance gap is small for smaller model which increases with size. \aayush{@ZF, if we have time, we should mention the shuffle rate for these plots. Also, the scale is not good, could have probably shown y axis between like 0.5 yto 0.9 to magnify the gap.}
% \fzx{agree}




\subsection{Probing Representations}
% Probing via Probing LLM's Evolving Understanding
\label{sec:probing}

\begin{figure}[h]
\centering
% \includegraphics[width=0.55\textwidth]{fig/ori token rank - sub token rank-amazon bijective_substitution half.png}
\includegraphics[width=0.49\textwidth]{fig/Rank_Difference_(Original_-_Substituted)_for_Layers_16-31_-_amazon_bijective_Substitution.pdf}
\caption{Rank difference (original token rank - substitution token rank) for \llama with bijective substitution on Amazon. Positive values (red) indicate the model's preference for substituted tokens over original ones. The visualization reveals that \textbf{as layers deepen, the model gradually deciphers and favors substituted tokens}.}
\label{fig:heatmap}
\end{figure}



% \begin{figure}[h]
% \centering
% \includegraphics[width=0.4\textwidth]{fig/ori token rank - sub token rank-sst_2 chunks.png}
% \caption{original token rank minus substitution token rank of the last layer on SST-2, displayed in chunks of size 3}
% \end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{fig/rank_difference_sst_2_chunks.pdf}
\caption{Average rank differences (original token rank - substitution token rank) in SST-2 for \textsc{Bijective} (blue) and \textsc{Non-bijective} (red) Cipher strategies over 15 occurrences, divided into 5 chunks of size 3. Rank difference serves as a proxy for the model's deciphering ability. Under bijective substitution, this ability improves with more exposure to substituted tokens, while random substitution shows no clear pattern.}
\end{figure}\label{fig:chunks}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.4\textwidth]{fig/ori token rank - sub token rank-amazon chunks.png}
% \caption{original token rank minus substitution token rank of the last layer on Amazon, displayed in chunks of size 3}
% \end{figure}



% \begin{figure}[h]
% \centering
% \includegraphics[width=0.45\textwidth]{fig/amazon_8b_70b_informative_performance_comparison.pdf}
% \caption{Performance comparison of Llama-3.1-8B and Llama-3.1-70B models on Amazon datasets (informative sampling) under Bijection and Random substitution strategies. The larger model slightly outperforms the smaller one and bijection substitution yields higher accuracy than random substitution.}
% \label{fig:model_size_amazon}
% \end{figure}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.45\textwidth]{fig/sst-2_8b_70b_informative_performance_comparison.pdf}
% \caption{Performance comparison of Llama-3.1-8B and Llama-3.1-70B models on SST\_2 datasets(informative sampling) under Bijection and Random substitution strategies.}
% \end{figure}



So far, our experiments have shown that LLMs are capable of deciphering substitution ciphers via ICL on multiple downstream tasks.
To understand the model's internal processing of ciphered inputs, we use Logit Lens \cite{logitlens} to probe LLMs's intermediate layer representations.
Logit Lens uses embeddings of a token from some intermediate layer, and uses the final LM head to decode it as the next token (representing what the model understands at that position).
% takes the embeddings of the $i$-th token\muhan{or "a specific token"? $i$ come from nowhere} from different layers as inputs, then multiply them with the LM head (unembedding matrix), and finally compute the prediction distribution (logits) for the next token.

In this section, we focus on probing on the sentiment classification task and use the same prompt in \ref{subsec:prompt}.
\paragraph{Selecting tokens for probing:}
We first pick 600 most frequent tokens in the demo set after filtering out tokens other than verbs, nouns and adjectives, using nltk \cite{bird2009natural}.
% \footnote{Filtering details are in \S\ref{appendix:filter}}
Then, we randomly sample 30 tokens from them as the ``original tokens''.
We then randomly sample another 30 tokens from the remaining 570 tokens as the ``substitution tokens'', each of which has a one-to-one correspondence with the original tokens.
% \muhan{from these, we randomly select 30 tokens as 'original tokens' and another 30 from the remaining 570 as 'substitution tokens', establishing a one-to-one correspondence between them.}
\paragraph{Token substitution:}
We use our two kinds of ciphers, namely,  \textsc{Bijective} and \textsc{Non-bijective}, in this probing experiment as well.
% \muhan{I changed the format here to make this easier to read}
For \textsc{Bijective} cipher, we create a bijection between the 30 ``original tokens'' and the chosen 30 ``substitution tokens'', creating a correspondence for the original tokens to be substituted.
For \textsc{Non-bijective} cipher, we substitute each occurrence of each original token, by a randomly sampled token from the remaining 570 tokens.
% (3) \underline{No Substitution} where we don't change the original tokens.
% bijective, random and no substitution.
% For bijective substitution, we substitute the original tokens with corresponding substitution tokens.
% For random substitution, each occurrence of original tokens is substituted by a randomly sampled token from the remaining 570 tokens.
% For no substitution, we don't change the original tokens.

\paragraph{Building ciphered inputs: }
Fr each ``original token'', we sample 15 examples that contain it from the demo pool, and apply our two substitution ciphers to build the ciphered input/prompt.
Given the positions of original tokens $P = \{p_1, p_2, ..., p_n\}$, we apply the Logit Lens and observe embeddings at $P' = \{p_1-1, p_2-1, ..., p_n-1\}$, i.e., one position prior, to find the ranks of ``original tokens'' and ``substitution tokens'' in the logits. This would give us an understanding of how the model changes its preference between original and substituted tokens. Now we talk about the results of our experiments.

\paragraph{Model understands \textsc{Bijective} ciphers:}\autoref{fig:heatmap} illustrates the rank difference (original token rank - substitution token rank) for \llama with \textsc{Bijective} substitution on Amazon dataset. Here, rank denotes the position of a given token in the model's softmax score over the vocabulary set.
\aayush{I am not confident in making any claim over shallow layers, so would remove the following point. commented out right now.}
% As the layer gets deeper, the rank difference become positive from negative (red from blue), which means the models given a higher rank to the ``substitution tokens'' than the  ``original tokens''.
% This suggests that the model's understanding of this substitution gets better with more illustrations of the substitution.
We observe that as the model sees more and more illustrations of the substitutions, the rank difference changes from positive to negative (in deeper layers, as that is where the model representations start taking the form of the intended output token). The model gives a higher rank to the ``substitution tokens'' than the  ``original tokens'', suggesting that the model starts to understand the substitutions. In contrast, there is no trend for \textsc{Non-bijective} ciphers as there is nothing to decipher. \aayush{I would highly recommend adding that figure here to make this contrast clear. I don't even see it in the appendix.}

To get a clearer vision, we extract the rank difference from the last layer on SST-2, dividing them equally into 5 chunks, as shown in \autoref{fig:chunks}. \aayush{is this necessary? its just showing the same thing in a different plot and might create more confusion in trying to understand the chunks. i would recommend removing it.}
For random substitution, there is not much change for rank difference.
For bijective substitution, rank difference increase as the chunk number gets bigger.
This suggests that as LLM sees more  occurence of the substitution token, it learns to use substitution token as the original token, namely soloving \name.
% We focus
% informative sampling
% task can change
% most popular 30 tokens, limited to noun, adj for understanding

% To better understand the deciphering phenomenon observed in the output of models, we conduct interpreability analysis on the representation of LLMs.
% Specifically, we build on the ``Logit Lens'' \cite{logitlens}, an early
% exiting technique that directly decodes hidden states into
% vocabulary space using the model's pretrained unembedding matrix.

% \daniel{TODO: Explain the setup}

% We note that several recent work~\cite{belrose2023eliciting,nguyen2024logit} also build upon Logit Lens to further improve its quality. We did not find any concrete reason to justify using these modified, but complex probing frameworks instead of Logit Lens.


\section{Discussion}

\paragraph{Didn't we already know that ICL does TL?} ... we provide a systematic way to quantify .. .


\paragraph{Can your results might be due to data contamination?}
Our work is actually motivated by issues such as data contamination that makes it difficult to attribute the success of ICL to ``retrieval'' (from pre-training) vs ``learning'' (from in-context demonstrations, without seeing them a priori). A reasonable approach to measure the latter (and mitigate the former) is through randomized tasks. The point of our study is to substitute the given tasks with randomly generated bijection tokens which makes it impossible for any model to have memorized them. We report the \textit{difference} in performance with bijection vs random shuffling and de-emphasize any absolute performance numbers which could have resulted from memorization of the original task. Even memorized tasks would be affected by token shuffling and the model would have to use Task Learning to decipher the substitution. We will make sure to emphasize this point in the text.

\paragraph{Does Bijective cipher guarantee measuring only ``learning''?} Not really ..

\paragraph{Do the small gains of Bijective cipher indicate that the weakness of learning ``learning'' in ICL?} Not really... task difficulty

\section{Conclusion}
\label{sec:conclusion}

We introduced \name, a class of cryptography text transformations designed to evaluate novel task learning capabilities of LLMs. To perform above baseline on these ciphered tasks, the LLM has to decipher a randomly generated but reversible key. We show that LLMs exhibit the capacity to decipher these novel tasks during inference. This evidence consolidates LLMs' ability to learn novel tasks outside of their pre-training corpus. The exact mechanism of this ``learning'' remains an active area of study. Understanding this mechanism holds the potential to make LLMs innovate solutions to unsolved problems.


\section*{Limitations}
Our work has several limitations that merit discussion:

\paragraph{Isolating TL:} Although \name{} aim to remove and practically hurt TR capabilities of LLMs, it is unclear if they are sufficient to isolate purely TL capabilities. We defer this exploration to future work.

\paragraph{Deviation from Natural Language:}
Ciphered text generated using \name{} diverges from natural language. While this is useful to assess LLMs' TL capabilities, it may also make the task excessively challenging for them. Therefore, we focus on the performance gaps (between \textsc{Bijective} and \textsc{Non-bijective} ciphers) instead of their absolute values.
% \muhan{I don't know if this paragraph is logical}

% Given that ciphered text generated using \name{} diverges from natural language, it provides insight into LLMs' TL capabilities. Meanwhile it may also make the task excessively challenging for them, resulting in difficulty of interpretation. Our focus on performance gap(between \textsc{Bijection} and \textsc{Non-bijective}), rather than their absolute values, offer comparative insights but may also overlook valuable information. The absolute performance values could potentially reveal important aspects of model behavior and limitations when processing highly unnatural text.

\paragraph{More models and datasets:}
Although we evaluated 16 settings (four models $\times$ four datasets), expanding our study to include more and larger models would strengthen our findings. The largest model we tested was Llama 3.2 70B, due to lack of more compute resources. Additionally, we did not evaluate aligned models such as GPT-4, O1, or Gemini. Anecdotal evidence suggests that aligned models may lose their ability to follow in-context demonstrations~\cite{fu2022does}, a crucial aspect of our task definition. However, we acknowledge that our task could potentially be adapted into a task description or instruction format suitable for aligned models, which deviates from our current setting and could be explored in future work. It would also be interesting to evaluate \name{} on various pre-training checkpoints to better understand how ICL ``learning'' emerges through pre-training.

\paragraph{More sophisticated interpretability analysis:}
In terms of interpretability analysis, we experimented with several approaches but found success only with the simplest method, the Logit Lens. More advanced interpretability analyses could provide deeper insights into the underlying mechanisms, offering a clearer understanding of the processes involved.

We recognize these as areas for further exploration and encourage future research to address these limitations.




% \section*{Limitations}
% Our paper has several limitations that warrant discussion:

% \paragraph{Deviations from natural language:}
% \name{} deviates from natural language, which might be necessary to quantify the ability of LLMs, but it also means that it might be excessively difficult for LLMs.
% This is why, we focus more on the performance gaps (between \textsc{Bijection} and \textsc{Non-bijective}), rather than their absolute values.

% \paragraph{More models and datasets:} While we have evaluated four open-source models and four datasets, expanding this to include more and larger models would enhance the robustness of our findings. The largest model we tested was the Llama with 70 billion parameters, as we lacked the resources to run the largest Llama model. Additionally, we did not evaluate aligned models such as GPT-4, O1, or Gemini. There is anecdotal evidence suggesting that aligned models may lose their ability to follow in-context demonstrations, a crucial aspect of our task definition. However, we ackowledge that our task could potentially be adapted into a task description or instruction format suitable for aligned models, which is essentially is a deviation from our setting here and could be explored in future work. It'd also be interesting to also evaluate \name{} on various pre-training checkpoints to better understand how ICL ``learning'' emerge through pre-training.

% \paragraph{More sophisticated interpretabiltiy analysis} In terms of interpretability analysis, we experimented with several approaches but found success only with the simplest method, the Logit Lens. Further interpretability analyses could provide deeper insights into the underlying mechanisms at play, offering a clearer understanding of the processes involved.

% We recognize these as areas for further exploration and encourage future research to address these limitations.


% (i) deviate from natural language (though that might be neccisty to quantity the ability
% (ii) we have already evaluated four open-source models and  and four datasets, but obviously it's be better to repreoduce on more and larger models.
% The larger model that we havr tried is Llama 70B parameter, since we did not have the resource to run the largeist llama model.
% We also did not try  aligned models such as GPT-4, O1 or Gemeni since there is various annecdotal evidence that aligned models lose their ability to follow in-context demonstrations, which are necessary aspect of our definition. Nevertheless, one can clearly turn our task into task descriptiom/instruction that can be fed into aligned models, though that's likely a different/followup work.
% We acknowledge that these are dimensions that one can explore.
% (iii) For our interpretability analysis also, we tried several approaches but only the simplest (Logit Lens) worked for us.  It'd be interesting to do more interpretability analysis to better understand what the hell is going on.

% \section*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \section*{Acknowledgments}
% This work is supported by a generous gift the Allen Institute for AI and partly by ONR grant (N00014-24-1-2089).
% We are grateful to ?? for their insightful feedback throughout this project. GPU machines for conducting experiments were provided by the DSAI and Rockfish clusters.

\newpage
\bibliography{ref}


\appendix
\onecolumn


\begin{center}
% \onecolumn
{\Large \textbf{Supplemental Material}}
% \twocolumn
\end{center}


% \appendix
% \section{Appendix}
\section{Additional Experimental Details}
\subsection{Preserved Tokens}
\label{appendix:reserved}
For Llama 3.1, we preserve the tokens whose ids range from 0 to 255, 128000 to 128256.
For Qwen 2.5, we preserve the tokens whose ids range from 0 to 255, 151643 to 151664.
For OLMo, we preserve the tokens whose ids range from 0 to 244, 50254 to 50279.
For Gemma 2, we preserve the tokens whose ids range from 0 to 472, 255968 to 255999.
For all the models, we preserve the spaces and underlines to ensure the framework of each task.
For example, in the WinoGrande dataset, LLMs are asked to predict the pronouns in a sentence, where the original pronouns are replaced by a underline.

\subsection{Handling of White Space}
\label{appendix:space}
LLMs encode the spaces between words differently depending on their tokenization.
\gemma uses a special underline to represent a space, while \llama, \qwen and \olmo uses 'Ġ'.
There are usually two versions of the same word -- with or without a space before it, which corresponds to two different tokens.
Take \llama for example, the encoded id of ``is'' is 285 while that of ``Ġis'' is 374.
We name tokens containing a space at the beginning as ``space tokens'' and the others as ``non-space tokens''.
To avoid disturbing spaces in the original text, which may confuse the model, we constrain the shuffling to be within their space/non-space sets.

% \subsection{Informative Sampling}
% \label{appendix:informative-sample}
% In informative sampling, we sample $n$ examples for text that has $m$ substituted tokens.
% If $m<n$, we randomly sample $n-m$ more examples other than the $m$ examples.
% If $m>n$, we randomly pick $n$ examples from the $m$ sampled examples.

% \subsection{Filtering}
% \label{appendix:filter}
% We use nltk \cite{bird2009natural} to filter tokens other than verbs, nouns and adjectives.

\definecolor{lightergray}{RGB}{230,230,230}
\definecolor{DarkRed}{RGB}{130,25,0}
\definecolor{DarkGreen}{RGB}{30,130,30}



\newcommand{\cmark}{\multirow{1}{*}{\textcolor{DarkGreen}{\ding{51}}}}
\newcommand{\xmark}{\multirow{1}{*}{\textcolor{red}{\ding{55}}}}


\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{>{\raggedright\arraybackslash}p{0.16\textwidth}>{\raggedright\arraybackslash}p{0.40\textwidth}>{\raggedright\arraybackslash}p{0.40\textwidth}}
    \toprule
    % \hline
    \textbf{Strategies for ...} & \textbf{Variant 1} & \textbf{Variant 2} \\
    \midrule
    selecting (sampling) demonstrations & \textbf{Priority}: select demonstrations that contain the target substitution in the test example~\cmark & \textbf{Non-priority}: select demonstrations randomly~\xmark \\
    \midrule
    % shuffling vocabulary in $S$ via $c$
    choosing the token mapping $c$
    & \textbf{Zipfian}:
    $c$ maps tokens of similar frequency (popularity) among each other~\cmark
    % shuffle tokens with similar frequency among each other
    % the token map $c$ shuffles tokens of similar frequency~\cmark
    & \textbf{Non-Zipfian}: $c$ maps tokens irrespective of their frequency (popularity)~\xmark \\
    % \midrule
    % ciphering (shuffling) tokens & \textbf{Bijective}: tokens (in the in-context demos) get replaced according to the reversible mapping $c$. & \textbf{Non-bijective}: Tokens (in the in-context demos) get replaced with a random (non-reversible) token. \\

    \bottomrule
    \end{tabular}
    \caption{Design choices for experiments in \name{} discussed in \S\ref{subsec:empirical}. }
    \label{tab:design_elements}
\end{table}

\subsection{Datasets}
\label{appendix:datasets}

For SST-2, HellaSwag and WinoGrande no label provided for the test set. Therefore, we use their validation set instead.

\paragraph{SST-2:}
We use its validation set as our test set, which has size of 872.
% which contains 444 positive examples and 428 negative examples (872 examples in total).
Its training set, which contains 67.3k examples, is used as the demo pool.
% 38K positive examples and 30k negative examples
\paragraph{Amazon:}
To fit the Amazon dataset into binary sentiment classification framework, we filter ratings 4-5 as positive and 1-2 as negative (discard rating 3).
We focus on reviews under the the ``All\_Beauty'' category in our experiments. We sample 144k positive and negative samples to build the demo pool; and 500 other positive and negative examples as the test set.
% We filter examples that contain the same reviews in test set from the rest
% \aayush{$\leftarrow$ what does this mean?}\fzx{there are examples in the demo pool that are exactly the same as the test set}
% and randomly sample 144k positive and 144k negative examples as the demo pool.
% \fzx{need help condenseing this}

\paragraph{HellaSwag:}
We use its validation set as our test set, which contains 444 positive examples and 428 negative examples (872 examples in total).
Its training set, which contains 38K positive examples and 30k negative examples, is used as the demo pool.

We randomly sample 1k examples from the validation set as our test set.
We use its training set as the demo pool, which contains 40k examples.



\paragraph{WinoGrande:}

We use its develop set as the test set, which contains 1267 examples.
Its xl training set is used as demo pool, which has 40k examples.
% \fzx{i think we should say it somewhere else}\aayush{agree, moving.}



\subsection{Prompt Template}
\label{subsec:prompt}
We don't include any instructions in our prompt.
For SST-2 and Amazon, we use the following prompt template:

\textbf{Input:} \{\textit{input\_demo}\}

\textbf{Output:} \{\textit{label\_demo}\}

...

\textbf{Input: }\{\textit{input\_test}\}

\noindent where \{\textit{input\_demo}\} and \{\textit{label\_demo}\} are the input text and sentiment labels of demonstrations, and \{\textit{input\_test}\} is the input text of test case.

For HellaSwag and WinoGrande, we use the following prompt template:

\textbf{Question:} \{\textit{question\_demo}\}

\textbf{Options:} \{\textit{options\_demo}\}

\textbf{Answer:} \{\textit{answer\_demo}\}

...

\textbf{Question:} \{\textit{question\_test}\}

\textbf{Options:} \{\textit{options\_test}\}

\noindent where {\textit{question\_demo}\}, {\textit{options\_demo}\} and \{\textit{answer\_demo}\} are the questions, options and correct answers of demos, and {\textit{question\_test}\} and {\textit{options\_test}\} are the question and option of the test case.



\section{Example Inputs/Outputs}
\label{appendix:examples}
% \daniel{Make sure to include examples of your prompts here. }
Here we display the example inputs/outputs on all the four datasets. Note that in our experiments the original inputs are not included in the prompts.
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=\textbf{
Dataset: SST-2; Model: \qwen; Cipher: bijective; Shuffle Rate: 0.6
}]
{
\tt
\textbf{Ciphered Input:} been sc Mil Swift the Inch for pen Venezuela Moody  \\
\textbf{Original Input:} been sent back to the tailor for some major alterations\\
\textbf{Output:} negative\\
 \\
\textbf{Ciphered Input:} is born Slovenia of an Platform San sitcom involved also Sr implementedecture embarrassed Swift Malay you reach for the tissues Confederate  \\
\textbf{Original Input:} is born out of an engaging storyline , which also is n't embarrassed to make you reach for the tissues . \\
\textbf{Output:} positive \\
... \\

\textbf{Ciphered Test Input:} allows us Swift hope Esc implementedolan Sr poised Swift cheating a Venezuela career Mr a assembled Kann steak filmmaker Confederate   \\
\textbf{Original Test Input:} allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \\
}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=\textbf{
Dataset: Amazon ; Model: \gemma ; Cipher: bijective; Shuffle Rate: 0.6
}]
{
\tt
\textbf{Ciphered Input:} didnSUwell really notice anything mob.  I sink it householder substance Woodward Bean Simple Woodward Senior Caldwell Snowyyn Ato was instance.   \\
\textbf{Original Input:} didn't really notice anything special.  I bought it because of the reviews and the price but honestly, I was disappointed. \\
\textbf{Output:}negative \\
 \\
\textbf{Ciphered Input:}Item arrived regions principle unrest neighbours']modern /><modern urchatosyn Woodward item was calcium steamer principle Counter cap rendering Woodward cover ent since it periodsSUwell Fam Arch anymore Simple iconicBer bottom Simple consequently']modern /><modern urchofficial was wrapped dentist regions principle padded envelope.   \\
\textbf{Original Input:}Item arrived in a quick manner.<br /><br />However, the item was received with a damaged cap rendering the cover useless since it won't snap on anymore and dented bottom and top.<br /><br />It was wrapped tightly in a padded envelope.  \\
\textbf{Output:}negative  \\
... \\

\textbf{Ciphered Test Input:} tried it for cosmetic qualifications perimeter a day spa\u00f2 didnPervers Tehran workil   \\
\textbf{Original Test Input:} tried it for cosmetic procedures in a day spa; didn't really work.  \\
}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=\textbf{
Dataset: Hellaswag; Model: \olmo ; Cipher: bijective; Shuffle Rate: 0.3
}]
{
\tt
\textbf{Ciphered Question:} Ter Back sits million titled with his Board effective on the keys. the Back    \\
\textbf{Original Question:} A man sits a piano with his hands placed on the keys. the man\\
\textbf{Ciphered Options:} (1) begins playing the titled.\textbackslash n(2) Carlos the keys with million malorn.\textbackslash n(3) beats the titled in million benefitedmic thought.\textbackslash n(4) increases the play for playing.\textbackslash n  \\
\textbf{Original Options:} (1) begins playing the piano.\textbackslash n(2) hits the keys with a mallet.\textbackslash n(3) beats the piano in a rhythmic beat.\textbackslash n(4) increases the volume for playing.\textbackslash n \\
\textbf{Answer:} (1) \\
 ...\\
\textbf{Ciphered Question:} People are noted on the street. million Back   \\
\textbf{Original Question:} People are running on the street. a man \\
\textbf{Ciphered Options:} (1) is wearing poetilts.\textbackslash n(2) limited million drink out Wars million After presidents.\textbackslash n(3) negotiating into million encourages Wars fire.\textbackslash n(4) limited million high jump in million Chris competition.\textbackslash n   \\
\textbf{Original Options:} (1) is wearing stilts.\textbackslash n(2) takes a drink out of a water bottle.\textbackslash n(3) jumps into a pile of fire.\textbackslash n(4) takes a high jump in a bar competition.\textbackslash n   \\
}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=\textbf{
Dataset: WinoGrande ; Model: \llama ; Cipher: bijective; Shuffle Rate: 0.3
}]
{
\tt
\textbf{Ciphered Question:} Estonia ferry that my parents story tied I permanent in Johnston permanent Stadium partners bla than my house now because the \_ permanent anchored.   \\
\textbf{Original Question:} The home that my parents had when I was in school was a lot nicer than my house now because the \_ was sophisticated. \\
\textbf{Ciphered Options:} (1) ferry, (2) house   \\
\textbf{Original Options:} (1) home, (2) house \\
\textbf{Answer:}(1) \\
 ...\\
\textbf{Ciphered Question:} Sarah permanent Stadium much better Chart than Maria so \_ always got the easier cases.   \\
\textbf{Original Question:} Sarah was a much better surgeon than Maria so \_ always got the easier cases. \\
\textbf{Ciphered Options:} (1) Sarah, (2) Maria   \\
\textbf{Original Options:} (1) Sarah, (2) Maria   \\
}
\end{tcolorbox}

\section{Additional Related Work}

\paragraph{Empirical understanding of ICL:} Ever since In-Context Learning was discovered~\citep{brown2020language}, multiple works have studied it under diverse settings~\citep{zhao2021calibrate,min2022rethinking,mishra2022reframing,han2023understanding,wang2023selfinstruct,sia2024does,vacareanu2024words,mueller2024context}.
For instance, \citet{srivastava2023beyond} benchmarked ICL under multiple tasks and models; \citet{perez2021true,Lu2022FantasticallyOP} showed the sensitivity of ICL to the choice of demonstrations and their orderings; \citet{shin2022effect,razeghi2022impact} showed the sensitivity of ICL performance to the frequency and size of the relevant pre-training corpus.
% ; and \citet{wei2022chain} used chain-of-thoughts to extract more performance out of language models.
These works have made useful observations that allow us to better use this elusive quality of LLMs.



\paragraph{Functional nature of ICL:} A more recent line of study aims to understand how ICL actually works in LLMs.
Multiple works have compared ICL with implicit optimization (specifically gradient descent)~\citep{garg2022can,zhang2023trained,dai2022can,akyurek2022learning,von2023transformers,li2023closeness,kim2024transformers}. This line of work claims that Transformers can meta-learn to perform optimization of internal models given a set of demonstrations. However, their study setup with toy transformers does not align with how LLMs are trained as shown by~\citet{shen2024icl_vs_gd}. Moreover, this line of study does not explain the TR capabilities of LLMs.

\paragraph{Forces that lead to ICL:} Few works try to understand \emph{how ICL emerges in LLMs}. \citet{xie2021explanation} explained ICL as implicit Bayesian inference, which maps a ICL demonstrations to a latent concept (task) learned via pre-training. \citet{hahn2023theory} posited that compositional structure in natural language gives rise to emergent in-context learning. Other works~\citep{chan2022data} theorize more distributional properties in the pre-training data, that may give rise to ICL. Many of these works explain some properties of ICL, but fail at others.
% For example, the Bayesian perspective from \citet{xie2021explanation}, does not explain how ICL can work even with random labels~\citep{min2022rethinking}. \aayush{is this sentence ok to say?}
The exact origin of ICL in LLMs still remains an active area of study.


\section{Additional Experiment Results}

\subsection{Informative\muhan{Here} vs Random Sampling}
\label{appendix:infor-random}
\autoref{fig:sst-curve-random} shows peformance of LLaMa 3.1 8B on SST-2 dataset with random sampling.
Comparing with \autoref{fig:sst-curve}, they demonstrate similar trend but there are more variance in random sampling due to the its random nature.
Therefore, we use informative\muhan{Here} sampling throughout our experiments for more steady results.
\begin{figure*}[h]
\centering
\includegraphics[width=0.48\textwidth]{fig/SST-2 llama3 Bijective Sub and Random Sampling.pdf}
\includegraphics[width=0.48\textwidth]{fig/SST-2 llama3 Gap with Random Sampling.pdf}
\caption{
\textbf{Left:} Peformance of LLaMa 3.1 8B on SST-2 dataset with random sampling. \textbf{Right:} The y-axis
displays the accuracy gap between Bijective and Random ciphers.
}\label{fig:sst-curve-random}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.48\textwidth]{fig/Amazon llama3 Bijective Sub and Informative Sampling.pdf}
\includegraphics[width=0.48\textwidth]{fig/Amazon llama3 Gap with Informative Sampling.pdf}
\caption{LLama3 accuracy on Amazon dataset with informative\muhan{Here} sampling (similar to Fig.\ref{fig:sst-curve}). The left plot shows the accuracy change of bijective cipher.
The right plot shows the gap between bijective and random cipher.}\label{fig:amazon-curve}
\end{figure*}


\begin{figure}[h]
\centering
% \includegraphics[width=0.55\textwidth]{fig/ori token rank - sub token rank-amazon bijective_substitution.png}
\includegraphics[width=0.49\textwidth]{fig/Rank_Difference_(Original_-_Substituted)_-_amazon_bijective_Substitution.pdf}
\caption{Whole heatmap of original token rank minus substitution token rank on Amazon. We show the partial heatmap for a subset of the layers in Fig.\ref{fig:heatmap}. }
\label{fig:heatmap:full}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{amazon_8b_70b_random_performance_comparison.pdf}
\caption{Accuracy comparison of Llama-3.1-8B and Llama-3.1-70B models on Amazon datasets(random sampling) under Bijective and Random substitution strategies.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{sst-2_8b_70b_random_performance_comparison.pdf}
\caption{Accuracy comparison of Llama-3.1-8B and Llama-3.1-70B models on SST\_2 datasets(random sampling) under Bijective and Random substitution strategies.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{fig/rank_difference_amazon_chunks.pdf}
\caption{Average rank differences in Amazon for bijective (blue) and random (red) substitution strategies over 15 occurrences, divided into 5 chunks of size 3.}
\end{figure}

\end{document}
